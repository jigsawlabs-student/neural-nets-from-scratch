{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we would probably like to go directly towards using gradient descent to minimize our cost curve instead of using gradient descent to find the minimum of some random functions.  But doing so is not so simple.  \n",
    "\n",
    "When we ask the question of how a cost curve changes as we change our parameters $w_1$ and $w_2$, the problem is more indirect than we seen in the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing our Sigmoid Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that with our sigmoid neuron is really the application of two functions:\n",
    "\n",
    "The first is our *weighted input*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(w) = w_1x_1 + w_2x_2 + w_3x_3 + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the second is the $sigmoid function$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g(x) = \\frac{1}{1 + e^x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $x$ is the output of our weighted input $f(x)$.  So our sigmoid neuron is, $g(f(w))$ given the functions defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So part of the problem of gradient descent with our sigmoid neuron, seeing how, if we change one of the weights in $f(w)$, it alters the output in $g(f(w))$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome the chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have nested functions like this, the problem of finding the change in output given a change an input even more complicated.  Mathematically, the task is the following: \n",
    "\n",
    "Given a function $g(f(x))$, find the change in output of $g$ given a change in $x$. \n",
    "\n",
    "To understand the problem we can think of changing the value of $x$, as causing a chain reaction.  Altering $x$ has an effect on the output $f(x)$, and altering $f(x)$ has an effect on the output of $g(f(x))$.\n",
    "\n",
    "Let's see this problem of a chain reaction with a different example than that of our sigmoid neuron.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h(x) = (3x + 1)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the function $h(x)$ above, and express it as two separate functions, where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x) =  3x + 1$$\n",
    "\n",
    "$$g(y) =  y^2$$\n",
    "\n",
    "And we can then represent $h(x)$ as\n",
    "\n",
    "$$h(x) = g(f(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to find the derivative with respect to $x$ we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta h}{\\delta x} = \\frac{\\delta g}{\\delta f}*\\frac{\\delta f}{\\delta x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's solve these individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g(f) =  f^2$ and $\\frac{\\delta g}{\\delta f} =  2f$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) =  3x + 1$, and $\\frac{\\delta f}{\\delta x} =  6$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use these components to solve for $\\frac{\\delta h}{\\delta x}$.\n",
    "\n",
    "$\\frac{\\delta h}{\\delta x} = \\frac{\\delta g}{\\delta f}*\\frac{\\delta f}{\\delta x} = 2f*3$, and because $f = 3x + 1$, substituting we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta h}{\\delta x} = 3*2*(3x + 1) = 18x + 6$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.90000000000012"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return 3*x + 1\n",
    "\n",
    "def g(y):\n",
    "    return y**2\n",
    "\n",
    "(g(f(3.1)) - g(f(3)))/(3.1 - 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "18*3 + 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The chain rule with the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $g(x) =  w_1x_1 + w_2x_2 + w_3x_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll continue to skip the sigmoid function.  Instead let's just assume that directly this input predicts whether or a cell is cancerous or not.  And the loss is.  So $g(x)$ is directly our hypothesis function, and our loss function is still directly:\n",
    "\n",
    "* $J(\\theta) = (actual - expected)^2$ or to place it another way\n",
    "\n",
    "* $J(\\theta) = \\sum (y_i - g(x_i))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the derivative we can get rid of ignore the summation, it won't make any difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $J(\\theta) = (y - g(x))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And remember that $g(x)$ is our weighted input, so let's plug that back in:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\theta) = (y - (w_1x_1 + w_2x_2 + w_3x_3 + w_0) )^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remember we want to find how we can update each of our parameters $w$ such that we start at a random set of parameters $w$, and then descend along our cost curve $J$.  And to do so, we move in the direction of the negative gradient.  Doing so will point us in the direction that descends along our cost curve.\n",
    "\n",
    "So to move in the direction of the negative gradient, we need to find the partial derivative with respect to each parameter $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta J}{\\delta w_1} = 2*(y - (w_1x_1 + w_2x_2 + w_3x_3 + w_0))*-x_1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta J}{\\delta w_2} = 2*(y - (w_1x_1 + w_2x_2 + w_3x_3 + w_0))*-x_2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And because it's the negative gradient, we reverse the signs of the partial derivatives to get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta J}{\\delta w_1} = 2(y - (w_1x_1 + w_2x_2 + w_3x_3 + w_0))*x_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta J}{\\delta w_2} = 2*(y - (w_1x_1 + w_2x_2 + w_3x_3 + w_0))*x_2 $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
