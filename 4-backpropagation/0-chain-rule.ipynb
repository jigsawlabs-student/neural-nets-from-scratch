{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we would probably like to go directly into using gradient descent to find the weights and biases of a neuron that minimize our cost curve. But doing so is not so simple. \n",
    "\n",
    "Remember that with gradient descent, we find the change in our function's output as we alter each parameter, and then step in in the steepest descent.  The issue with applying this technique is that the impact on the output is more indirect.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing the issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous lessons, we found the rate of change in our output as we changed the parameters in a function like:\n",
    "\n",
    "$$z(w,b) = 3w + b$$\n",
    "\n",
    "To find how the output of $f$ changes as we change each parameter, we take the partial derivative with respect to each parameter, above $w$ and $b$.  But when trying to find the parameters that minimize a cost curve, this time we are not finding the impact of altering the parameters on that same function $f$, but on another function, our cost function: \n",
    "\n",
    "$$J(w,b) = (y - z(w, b))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For right now, we are leaving off the summation, just to keep our function a little less intimidating.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we said this is more indirect.  And really it's more indirect than even that, because $z(w, b)$ is just the linear component, which is then fed to the activation function $\\sigma$ to make the prediction, which is then passed into our cost function:\n",
    "\n",
    "$$J(w,b) = (y - \\sigma(z(w, b)))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we said, seeing how the changing the parameters $w$ and $b$ impact our cost curve is more indirect than we previously saw.  But don't worry, mathematicians have already figured out how to solve problems like the one above.  We just have to learn their approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing the Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem that we are running into above, is how to find the derivative of a composite function.  Our cost function is depends on our hypothesis function which is composed of the sigmoid and linear function\n",
    "\n",
    "$J(w, b) = J(y - \\sigma(z(w, b))$\n",
    "\n",
    "Let's see how we can find the derivative of composite functions with a simpler example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x) = (3x + 1)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Break it down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *In Math*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see how this is a composite function?  We start with the function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x) = (3x + 1)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we break this into two functions, $h(x)$ and $g(y)$ where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ g(y) = y^2$$\n",
    "$$h(x) = 3x + 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can rewrite our function, $f(x)$ as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x) = g(h(x)) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we broke our function $f(x)$ down above, by defining two functions $h(x)$ and $g(y)$, and then passing the output of $h(x)$ into $g(y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *In Code* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also translate breaking down the function $f(x) = (3x + 1)^2$ into code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f(x): return (3*x + 1)**2 is equivelent to:\n",
    "def f(x): \n",
    "    return g(h(x))\n",
    "\n",
    "def g(y): \n",
    "    return y**2\n",
    "\n",
    "def h(x):\n",
    "    return (3*x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Finding the derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have rewritten our function \n",
    "$$f(x) = (3x + 1)^2$$ as:\n",
    "\n",
    "$$f(x) = g(h(x)) $$  where:\n",
    "\n",
    "$$h(x) = 3x + 1$$\n",
    "$$ g(h) = h(x)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the derivative $f(x)$ with respect to $x$ we do the following:\n",
    "\n",
    "> take the derivative of the our function outer $g(h)$ and multiply it by the derivative of the inner function $h(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f'(x) = g'(h(x)) * h'(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or using our other notation: $\\frac{\\delta f}{\\delta x} = \\frac{\\delta g}{\\delta h}*\\frac{\\delta h}{\\delta x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find the derivatives of our two functions individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g(h(x)) =  h(x)^2$ and $g'(h(x)) =  2h(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h(x) =  3x + 1$, and $h'(x) =  3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plug these components in our formula $f'(x) = g'(h(x)) * h'(x)$.\n",
    "\n",
    "We get $f'(x) = g'(h(x)) * h'(x) = 2h(x)*3 = 6*h(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And because $h(x) = (3x + 1)$, substituting further we get: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta f}{\\delta x} = 6h(x) = 6(3x + 1) = 18x + 6 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we just did is pretty cool.  We were able to see how calculate how nudging the value of $x$ impacts a composite function $$f(x) = (3x + 1)^2$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculated this by breaking $f(x) = (3x + 1)^2$ into two functions, an outer function $g(h)$, and an inner function $h(x)$.\n",
    "\n",
    "$$ g(h) = h(x)^2$$\n",
    "$$h(x) = 3x + 1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then found $f'(x) = g'(h)*h'(x) = 2h(x)*3 = 6h(x) = 6*(3x + 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we found that: \n",
    "\n",
    "$f'(x) = 18x + 6 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make sure we can interpret what we found.  We calculated how the output of $f(x)$ changes as we nudge our value of $x$ at different points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for example, when $x = 3$, the rate of change of our function with respect to $x$ is $f'(x) = 18x + 6  = 18*3 + 4 = 60$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being able to find the derivative of composite function by applying the chain rule is critical because our cost function is for a neuron and indeed a neural network, is one big composite function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(w,b) = (y - z(w, b))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will need to calculate the instantaneous rate of change in our cost function as we change our parameters  -- that is our weights and our bias.  As we know we'll use this instantaneous rate of change to determine how to update our parameters so that we can find the parameters that minimize our cost function $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"https://www.jigsawlabs.io/free\" style=\"position: center\"><img src=\"https://storage.cloud.google.com/curriculum-assets/curriculum-assets.nosync/mom-files/jigsaw-labs.png\" width=\"15%\" style=\"text-align: center\"></a>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
