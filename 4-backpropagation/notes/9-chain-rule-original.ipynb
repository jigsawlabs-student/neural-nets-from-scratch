{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we would probably like to go directly into using gradient descent to find the weights and biases of a neuron that minimize our cost curve. But doing so is not so simple. \n",
    "\n",
    "Remember that with gradient descent, we find the change in our function's output as we alter each parameter, and then step in in the steepest descent.  The issue with applying this technique is that the impact on the output is more indirect.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing the issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous lessons, we found the rate of change in our output as we changed the parameters in a function like:\n",
    "\n",
    "$$z(w,b) = 3w + b$$\n",
    "\n",
    "To find how the output of $f$ changes as we change each parameter, we take the partial derivative with respect to each parameter, above $w$ and $b$.  But when trying to find the parameters that minimize a cost curve, this time we are not finding the impact of altering the parameters on that same function $f$, but on another function, our cost function: \n",
    "\n",
    "$$J(w,b) = (y - z(w, b))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For right now, we are leaving off the summation, just to keep our function a little less intimidating.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we said this is more indirect.  And really it's more indirect than even that, because $z(w, b)$ is just the linear component, which is then fed to the activation function $\\sigma$ to make the prediction, which is then passed into our cost function:\n",
    "\n",
    "$$J(w,b) = (y - \\sigma(z(w, b)))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we said, seeing how the changing the parameters $w$ and $b$ impact our cost curve is more indirect than we previously saw.  \n",
    "\n",
    "But don't worry, mathematicians have already figured out how to solve problems like the one above.  We just have to learn their approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing the Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem that we are running into above, is how to find the derivative of a nested function.  Our linear function is passed to our sigmoid function, whose output is then passed to the cost function.\n",
    "\n",
    "> *Look to wikipedia to see how can reword nested functions*\n",
    "\n",
    "Let's see how we can solve this with a simpler example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that with our sigmoid neuron is really the application of two functions:\n",
    "\n",
    "The first is our *weighted input*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(w) = w_1x_1 + w_2x_2 + w_3x_3 + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the second is the $sigmoid function$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g(x) = \\frac{1}{1 + e^x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $x$ is the output of our weighted input $f(x)$.  So our sigmoid neuron is, $g(f(w))$ given the functions defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So part of the problem of gradient descent with our sigmoid neuron, seeing how, if we change one of the weights in $f(w)$, it alters the output in $g(f(w))$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome the chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have nested functions like this, the problem of finding the change in output given a change an input even more complicated.  Mathematically, the task is the following: \n",
    "\n",
    "Given a function $g(f(x))$, find the change in output of $g$ given a change in $x$. \n",
    "\n",
    "To understand the problem we can think of changing the value of $x$, as causing a chain reaction.  Altering $x$ has an effect on the output $f(x)$, and altering $f(x)$ has an effect on the output of $g(f(x))$.\n",
    "\n",
    "Let's see this problem of a chain reaction with a different example than that of our sigmoid neuron.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h(x) = (3x + 1)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the function $h(x)$ above, and express it as two separate functions, where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x) =  3x + 1$$\n",
    "\n",
    "$$g(y) =  y^2$$\n",
    "\n",
    "And we can then represent $h(x)$ as\n",
    "\n",
    "$$h(x) = g(f(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to find the derivative with respect to $x$ we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta h}{\\delta x} = \\frac{\\delta g}{\\delta f}*\\frac{\\delta f}{\\delta x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's solve these individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g(f) =  f^2$ and $\\frac{\\delta g}{\\delta f} =  2f$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) =  3x + 1$, and $\\frac{\\delta f}{\\delta x} =  6$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use these components to solve for $\\frac{\\delta h}{\\delta x}$.\n",
    "\n",
    "$\\frac{\\delta h}{\\delta x} = \\frac{\\delta g}{\\delta f}*\\frac{\\delta f}{\\delta x} = 2f*3$, and because $f = 3x + 1$, substituting we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta h}{\\delta x} = 3*2*(3x + 1) = 18x + 6$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.90000000000012"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return 3*x + 1\n",
    "\n",
    "def g(y):\n",
    "    return y**2\n",
    "\n",
    "(g(f(3.1)) - g(f(3)))/(3.1 - 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "18*3 + 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The chain rule with the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $g(x) =  w_1x_1 + w_2x_2 + w_3x_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll continue to skip the sigmoid function.  Instead let's just assume that directly this input predicts whether or a cell is cancerous or not.  And the loss is.  So $g(x)$ is directly our hypothesis function, and our loss function is still directly:\n",
    "\n",
    "* $J(\\theta) = (actual - expected)^2$ or to place it another way\n",
    "\n",
    "* $J(\\theta) = \\sum (y_i - g(x_i))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the derivative we can get rid of ignore the summation, it won't make any difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $J(\\theta) = (y - g(x))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And remember that $g(x)$ is our weighted input, so let's plug that back in:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\theta) = (y - (w_1x_1 + w_2x_2 + w_3x_3 + w_0) )^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remember we want to find how we can update each of our parameters $w$ such that we start at a random set of parameters $w$, and then descend along our cost curve $J$.  And to do so, we move in the direction of the negative gradient.  Doing so will point us in the direction that descends along our cost curve.\n",
    "\n",
    "So to move in the direction of the negative gradient, we need to find the partial derivative with respect to each parameter $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta J}{\\delta w_1} = 2*(y - (w_1x_1 + w_2x_2 + w_3x_3 + w_0))*-x_1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta J}{\\delta w_2} = 2*(y - (w_1x_1 + w_2x_2 + w_3x_3 + w_0))*-x_2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And because it's the negative gradient, we reverse the signs of the partial derivatives to get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta J}{\\delta w_1} = 2(y - (w_1x_1 + w_2x_2 + w_3x_3 + w_0))*x_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta J}{\\delta w_2} = 2*(y - (w_1x_1 + w_2x_2 + w_3x_3 + w_0))*x_2 $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
