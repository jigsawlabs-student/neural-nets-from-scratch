{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson, we learned about initializing a layer to our neural network.  We did so by building a weight matrix where we specified the dimensions of the attributes of the layer - a column for each neuron, and a row for each feature that the neuron accepts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we'll move beyond constructing single layers and learn how to build a network with multiple layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by taking another look at our single layer matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./first-layer.png\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be a bit large to draw a picture of, but imagine that we really have a layer with 5 neurons, and that each observation has 10 features.  Remember that we want each neuron to accept each of these ten features. \n",
    "\n",
    "Ok, now let's initialize our weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.00036589, -0.38109252, -0.37566942, -0.07447076,  0.43349633])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2)\n",
    "W_1 = np.random.randn(10, 5)\n",
    "\n",
    "b_1 = np.random.randn(5)\n",
    "b_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this indicates that we start with an observation with four features, and each feature in the observation is passed to each neuron in the first layer.  Then each of those neurons has an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.28594759, 0.91737969, 0.00451515, 0.17750781, 0.32831314])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(value): return 1/(1 + np.exp(-value))\n",
    "\n",
    "x = np.array([.9, .4, .5, .6, .9, .8, .7, .2, .4, .3])\n",
    "sigmoid(x.dot(W_1) + b_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we get five different outputs ranging from 0 to 1, one for each of our neurons. \n",
    "\n",
    "Now to build second layer in our neural network, we take these five outputs from our first layer and feed these ouputs into each of the neurons in our second layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So think about what this means.  Our second layer can have as many neurons as we want, but the number of features of each neuron must equal the outputs - that is the number of neurons - from the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./two-layers.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that our first layer has five neurons, with each taking in an observation of ten features.  Each neuron outputs a single value, and together the layer outputs five values.  Those five values are the inputs to each of the three neurons in the third layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think about how we can represent this in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have the first layer and the corresponding biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So we confirmed our first layer has five neurons ten parameters each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this means that because our layer outputs a vector of size 5, the next layer must have five rows, with a column for each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_2 = np.random.randn(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_2 = np.random.randn(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can place these two layers into a single list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's play around with this a little bit.  If we want to execute the first layer, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.28594759, 0.91737969, 0.00451515, 0.17750781, 0.32831314])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_output = sigmoid(x.dot(W_1) + b_1)\n",
    "first_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then this is fed into the second layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.63303723, 0.54528456, 0.41825229])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(second_layer[0].dot(first_output) + second_layer[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great.  Now let's use a loop to clean this up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(input_features, layer):\n",
    "    W = layer[0]\n",
    "    b = layer[1]\n",
    "    return sigmoid(input_features.dot(W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2839813 , 0.29652172, 0.377956  ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features = x\n",
    "layers = [(W_1, b_1), (W_2, b_2)]\n",
    "for layer in layers:\n",
    "    input_features = feed_forward(input_features, layer)\n",
    "input_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that is how our neural network makes a prediction.  \n",
    "\n",
    "1. Initialize random weights and biases for each layer where the number of weights of each neuron are the columns, and there is a separate row for each neuron.\n",
    "\n",
    "2. Feed forward each layer with the formula $\\sigma(W \\cdot x + b)$, where x is the vector of inputs for the first layer, and afterwards is the vector of the previous layer's outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want our final layer to tell us the likelihood that our observation is each possibility.  So if we want our network to classify an observation as cancerous or benign, our last layer has two outputs.  If we would like our network to classify an image of one of twenty-six letters our last layer has 26 outputs (27 if we have a none of the above category).  So in the last layer, the number of neurons determines the number of outputs we predict.  So for a neuron predicting letters, we would have 26 neurons in the last layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
