{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson, we learned about initializing a layer to our neural network.  We did so by building a weight matrix where we specified the dimensions of the attributes of the layer - a column for each neuron, and a row for each feature that the neuron accepts.  For example, we constructed a linear layer with two neurons, each of which accepts five features as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.49368401,  0.12450703],\n",
       "       [-2.32335428, -0.19064423],\n",
       "       [ 0.4744999 ,  1.11803926],\n",
       "       [-0.54752483,  1.93996   ],\n",
       "       [ 0.26949592, -0.741873  ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "W = np.random.randn(5, 2)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.59001735, 0.09078673])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.random.randn(2)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we'll move beyond constructing single layers and learn how to build a network with multiple layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by taking another look at our single layer matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./first-layer.png\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's imagine that each observation in our training data has 10 features.  For example, our feature vector x, representing our first observation looks like the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([.9, .4, .5, .6, .9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a layer that takes in a feature vector of this size, we need a matrix of 10 rows.  And let's have a layer with five neurons, giving us five columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2)\n",
    "W_1 = np.random.randn(5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our bias vector has five entries, one for each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.87810789, -0.15643417,  0.25657045])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_1 = np.random.randn(3)\n",
    "b_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pass our first observation through the linear layer, and then the sigmoid function for the activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29866564, 0.09776106, 0.33822734])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(value): return 1/(1 + np.exp(-value))\n",
    "\n",
    "l1 = x.dot(W_1) + b_1\n",
    "a1 = sigmoid(l1)\n",
    "a1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that we get five different outputs ranging from 0 to 1, one for each of our neurons. \n",
    "\n",
    "Now if we want to add a second layer to our neural network, we take the five outputs from the neurons in our first layer and feed these ouputs to each neurons in our second layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is called a **fully connected** neural network.  In a fully connected neural network, every neuron in one layer connects to every neuron in another layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the number of features accepted by a neuron in one layer must equal the number of inputs it receives from the previous layer.  \n",
    "\n",
    "> (Or in case of the first layer, must equal the number of features of an observation.)  \n",
    "\n",
    "In contrast to the features, the number of *neurons* that composes each layer is not constrained by previous layers.  \n",
    "\n",
    "* Or to put this in terms of our weight matrix, the number of rows in a weight matrix is equal to the number of the neurons in the previous layer, as each neuron outputs a single number. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walking through an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see this with our example above.  We start with an observation's feature vector that has five entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9, 0.4, 0.5, 0.6, 0.9, 0.8, 0.7, 0.2, 0.4, 0.3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means our weight matrix must have five rows (one for each weight of the neuron).  And we previously specified that there be three columns, to represent the weights of three neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.41675785, -0.05626683, -2.1361961 ],\n",
       "       [ 1.64027081, -1.79343559, -0.84174737],\n",
       "       [ 0.50288142, -1.24528809, -1.05795222],\n",
       "       [-0.90900761,  0.55145404,  2.29220801],\n",
       "       [ 0.04153939, -1.11792545,  0.53905832]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the outputs of the neurons' linear components are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02444785, -2.0659189 , -0.92777425])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dot(W_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29866564, 0.09776106, 0.33822734])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(x.dot(W_1) + b_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we get an output from each of the three neurons, our next weight matrix, $l2$ must have three rows, one row for the weights of each neuron.\n",
    "\n",
    "> We'll initialize it to have two neurons.  Each column in the weight matrix represents a neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.77101174, -1.86809065],\n",
       "       [ 1.73118467,  1.46767801],\n",
       "       [-0.33567734,  0.61134078]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_2 = np.random.randn(3, 2)\n",
    "W_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> And there is a bias term for each vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04797059, -0.82913529])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_2 = np.random.randn(2)\n",
    "b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can initialize our weights and biases so that our layers are fully connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.9, 0.4, 0.5, 0.6, 0.9, 0.8, 0.7, 0.2, 0.4, 0.3])\n",
    "W_1 = np.random.randn(5, 3)\n",
    "b_1 = np.random.randn(3)\n",
    "\n",
    "W_2 = np.random.randn(3, 2)\n",
    "b_2 = np.random.randn(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we feed forward our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* feedforward because information ﬂows through the function being evaluated from x, through the intermediate computations used to deﬁne f, and ﬁnally to the output y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = sigmoid(x.dot(W_1) + b_1)\n",
    "l2 = sigmoid(l1.dot(W_2) + b_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize\n",
    "\n",
    "So that is how our neural network makes a prediction.  \n",
    "\n",
    "1. Initialize random weights and biases for each layer where the number of weights of each neuron are the rows, and there is a separate vector for each neuron.\n",
    "\n",
    "2. Feed forward each layer with the formula $\\sigma(W \\cdot x + b)$, where x is the vector of inputs for the first layer, and afterwards x is the vector of the previous layer's outputs\n",
    "\n",
    "3. For each weight matrix W, the rows represent the number of weights of each neuron and must equal the number of neurons in the previous layer (or for the first layer, the number of features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It gets kinda confusing because now we are calling each neuron a linear component."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
