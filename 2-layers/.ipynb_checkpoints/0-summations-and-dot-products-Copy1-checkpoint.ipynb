{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perspectives on a Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lessons we built up to our sigmoid neuron.  We saw that a neuron was a linear function wrapped in a non-linear activation function.  In this lesson, let's see a couple of ways of representing this with a summation notation and with a dot product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that our sigmoid neuron consists of a linearity and an activation function where:\n",
    "\n",
    "1. The **linear function** is: $z(x) = w_1x_1 + w_2x_2 ... w_nx_n + b $\n",
    "\n",
    "2. And the **activation function** is: $\\sigma(z)) =  \\frac{1}{1 + e^{-z(x)}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./sigmoid-neuron.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewriting our Linear Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know the general form of our linear function is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z(x) = w_1x_1 + w_2x_2 ... w_nx_n + b $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's just focus on the weighted sum (that is, every term except for $b$).\n",
    "\n",
    "$g(x) = w_1x_1 + w_2x_2 ... w_nx_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the function of the weighted sum is to take every feature of an observation, and weigh it accordingly in producing an output.  For example, let's return to our scenario of a neuron that judges whether something is sweet.  Our neuron weighs taste twice as much smell.  Let's place these weights in a list called $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [2, 1]\n",
    "# taste weight 2\n",
    "# smell weight 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's say the corresponding values for our first observation is $3$ and $4$, which we'll place into a list called $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in math terms, we call these two lists vectors, $w$, and $x$.  And the *dot product* of these two vectors, is pairing the elements of these vectors together by their index, multiplying the pairs, and then adding.  It looks like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "2 & 1 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "3 \\\\\n",
    "4 \n",
    "\\end{bmatrix} = 2*3 + 1*4 = 10$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point is that if we have a vector of weights $w$ and features $x$, then we can calculate the weighted sum with the dot product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g(x) = \\begin{bmatrix}\n",
    "w_1 & w_2 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \n",
    "\\end{bmatrix} = w_1*x_1 + w_2*x_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this by using the numpy library's dot function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([2, 1])\n",
    "w = np.array([3, 4])\n",
    "x.dot(w)\n",
    "# 2*3 + 1*4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so we can rewrite our linear function, with the bias as: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $z(x) = w_1x_1 + w_2x_2 ... w_nx_n + b $\n",
    "* $= w \\cdot x + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this allows us to rewrite the hypothesis function of our sigmid neuron as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8807970779778823"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(value):\n",
    "    return 1/(1 + np.exp(-value))\n",
    "\n",
    "sigmoid(x.dot(w) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or mathematically, we can express the hypothesis function of our entire sigmoid neuron as: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma(x \\cdot w + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's it, our linear function wrapped in our sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we learned about the dot product.  The dot product is the multiplication of two vectors.  And we used the dot product to rewrite the our linear function as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z(x) = w\\cdot x + b = \\begin{bmatrix}\n",
    "w_1 & w_2 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \n",
    "\\end{bmatrix} + b  = w_1*x_1 + w_2*x_2 + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also saw that we can calculat the dot product of two vectors in numpy by using the `dot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([2, 1])\n",
    "w = np.array([3, 4])\n",
    "b = -15\n",
    "w.dot(x) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from there, we can calculate the hypothesis function of the sigmoid neuron as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(value):\n",
    "    return 1/(1 + np.exp(-value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0066928509242848554"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(x.dot(w) + b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
