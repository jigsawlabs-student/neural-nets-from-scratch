{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Initial Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson, we saw that we can represent the weights of our neurons with a weight matrix, $W$.  In that matrix, each vector represents the weights of a different neuron.  So below, W contains the weights of two neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W = \\begin{bmatrix}\n",
    "|  & |  \\\\\n",
    "w_1  & w_2 \\\\\n",
    "|   & |\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we used the weight matrix to calculate outputs of the linear functions of each neuron, by multiplying our weight matrix by a feature vector x, and adding the bias of each neuron.\n",
    "\n",
    "> So below is the ouput of the linear component of the first layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = \\begin{bmatrix}\n",
    "- & x &  -  \n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "|  & |  \\\\\n",
    "w_1  & w_2 \\\\\n",
    "|   & |\n",
    "\\end{bmatrix} + \\begin{bmatrix} b_1 & b_2 \\end{bmatrix} = \\begin{bmatrix}\n",
    "x \\cdot w_1 & x \\cdot w_2 \\end{bmatrix} + \\begin{bmatrix} b_1 & b_2 \\end{bmatrix} = \\begin{bmatrix} h(x) & h(x) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Random Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now so far, the weights for each of our neurons were given to us.\n",
    "\n",
    "But as we'll see when move into the section on training, our neural network **does not know** which weights to begin with.  Rather, it learns these weights over time.  \n",
    "\n",
    "So for us, this means that we can just initialize our neural network with random weights, and the important thing is to get the dimensions of each neuron, and our layer correct.  Let's walk through this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move back to our domain of using neurons to predict if a cell is cancerous.  Let's say that for our first observation, we have a feature vector $x$, displayed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# perimeter, radius, volume, # asymmetries, bumpiness    \n",
    "x = np.array([2, 1, 1, 5, 4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to initialize a neuron that accepts each of these features, we simply need to initialize a vector with five random weights: one for each feature.  And initialize a random bias term.  We can do this with numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.06014071, -0.3224172 , -0.38405435,  1.13376944, -1.09989127]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "n_1_w = np.random.randn(1,5)\n",
    "n_1_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of points to note from above.\n",
    "\n",
    "1. The `randn` takes two arguments, the number of rows to return and the number of columns to return.  So above we call `randn(1, 5)` as we want a vector of one row and five columns.\n",
    "2. `Randn` draws values from the standard normal distribution.  This is a good starting point to initialize a weight matrix.  \n",
    "> If you're not familiar with this distribution, it's not critical to your understanding going forward.  The important point is that we will draw values centered around 0, and that 95% of numbers will be between -2 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's initialize the bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.17242821])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_1_b = np.random.randn(1)\n",
    "n_1_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we want to use this weight vector to take in our feature vector and make a prediction, it can do so with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(value): return 1/(1 + np.exp(-value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02343297785415176"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(x.dot(n_1_w[0]) + n_1_b[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we don't know if this prediction is any good, or even what this neuron is responsible for predicting.  But that's ok.  We'll worry about that when we discuss training.  The important thing is that we have one neuron that takes in our observation and predicts something.\n",
    "\n",
    "And we got that by initializing a random vector that had the same length as the feature vector, $x$, and a bias term consisting of a single value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a random layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we built a single neuron with two lines of code - one line to represent the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05080775],\n",
       "       [-0.63699565],\n",
       "       [ 0.19091548],\n",
       "       [ 2.10025514],\n",
       "       [ 0.12015895]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And another line of code to represent the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.74481176])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the code, `np.random.randn(5, 1)` we are constructing a matrix with five rows and one column.  The one column is for our single neuron and the five rows are for the five weights to correspond with the features of an observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Go further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now let's initialize weight matrix to represent two neurons, each with five weights.  Here it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.61720311,  0.30017032],\n",
       "       [-0.35224985, -1.1425182 ],\n",
       "       [-0.34934272, -0.20889423],\n",
       "       [ 0.58662319,  0.83898341],\n",
       "       [ 0.93110208,  0.28558733]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.random.randn(5, 2)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the bias is simply a vector with two entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2344157 , 1.65980218])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.random.randn(2)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can make predictions with these neurons with our normal function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00162605, 0.00313009])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(x.dot(W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we want to build the weights and biases for a neural network that looks like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./first-layer.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's simply four columns (one for each neuron) and four rows (each neuron has four weights) matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01791712,  0.20491942,  0.12166726,  0.85470516],\n",
       "       [-0.54014973,  0.20377669, -0.64654733,  1.51752847],\n",
       "       [-1.27656353, -0.42488659,  0.02028611, -0.53855715],\n",
       "       [-0.00312273,  1.6536728 , -1.13278799, -0.18014837]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "W = np.random.randn(4,4)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.51094515, -0.4406927 ,  0.58420857,  0.43645582])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.random.randn(4)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So notice that above we have four different neurons, each taking in the the four features of an observation x, and making a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we saw how we can use numpy to initialize a layer of a neural network.  We do with the `np.random.randn` function, and then specifying the specifying the dimensions of our layer.  For the weight matrix W, the number of rows corresponds to the length of the feature vector.  And there is one vector for each neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How many neurons to initialize with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make sure you don't say `length` to describe the feature vector.  It's number of entries or dimensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
