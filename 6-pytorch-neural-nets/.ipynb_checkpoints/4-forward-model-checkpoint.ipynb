{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Oriented Pytorch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lessons, we became familiar with tensors, linear layers, and activation functions in Pytorch to initialize a model and perform a forward pass of the data.  So far, things have been pretty similar to what we have seen in numpy.  We initialize some sample numbers for our weights and biases, and use functions as our activation layers.  \n",
    "\n",
    "We did this through two functions: an `init_model` function and a `forward` function.  Because these two functions relate to the same neural network, we might imagine combining them into a single class.  That is what we'll accomplish in this lesson.\n",
    "\n",
    "Later on, we'll see that combining these functions in a class, allows Pytorch to easily calculate our gradients when we eventually train our neural network.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Functions to Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have written two functions to initialize a model and pass through our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': Linear(in_features=784, out_features=64, bias=True),\n",
       " 'W2': Linear(in_features=64, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "def init_model():\n",
    "    W1 = nn.Linear(28*28, 64)\n",
    "    W2 = nn.Linear(64, 10)\n",
    "    return {'W1': W1, 'W2': W2} \n",
    "\n",
    "model = init_model()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, model):\n",
    "    W1, W2 = tuple(model.values())\n",
    "    Z1 = W1(X)\n",
    "    A1 = F.sigmoid(Z1)\n",
    "    Z2 = W2(A1)\n",
    "    A2 = F.softmax(Z2, dim = 1)\n",
    "    return (Z1, A1, Z2, A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now moving these two functions into a class really doesn't change our two functions too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(28*28, 64)\n",
    "        self.W2 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        A1 = torch.sigmoid(self.W1(x))\n",
    "        return F.softmax(self.W2(A1), dim = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main changes here.  First, we move our init_model function into the `__init__` function.  As we know from object orientation, this function will now be called when we initialize our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (W1): Linear(in_features=784, out_features=64, bias=True)\n",
       "  (W2): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ??nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we have our class inherit from our `nn.Module` function.  This adds functionality to our class, which we'll see later.\n",
    "\n",
    "Finally, we set up call the `__init__` function in the `nn.Module` class, through the `super().__init__()` init line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it.  We've successfully setup a neural network in Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Use of Our Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is the code we needed to translate our functions to a class in Pytorch.  Now let's see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        A1 = torch.sigmoid(self.fc1(x))\n",
    "        return F.softmax(self.fc2(A1), dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to initialize our neural network, and assign it to a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's pass some data through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First we initialize an instance with one row, 784 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.randn(1, 28*28)\n",
    "# x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we pass through our data to our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0882, 0.0702, 0.1430, 0.0827, 0.0477, 0.1715, 0.1099, 0.1002, 0.1011,\n",
       "         0.0855]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that passing through the data automatically calls the `forward` function in our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "        A1 = torch.sigmoid(self.fc1(x))\n",
    "        return F.softmax(self.fc2(A1), dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that we've completed our hypothesis function in Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll spend the next couple of lessons walking through training our Pytorch network, but because we've already set up our neural network, we might as well quickly walkthrough the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First let's download our MNIST datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 9871360/9912422 [00:16<00:00, 687022.00it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/train-images-idx3-ubyte.gz to MNIST/raw\n",
      "Using downloaded and verified file: MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST/raw/train-labels-idx1-ubyte.gz to MNIST/raw\n",
      "Using downloaded and verified file: MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST/raw\n",
      "Using downloaded and verified file: MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:30, 687022.00it/s]                             "
     ]
    }
   ],
   "source": [
    "train = datasets.MNIST(\"\", train = True, download = True, \n",
    "                       transform = transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(\"\", train = False, download = True, \n",
    "                       transform = transforms.Compose([transforms.ToTensor()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To group our observations into batches -- here of size 50 -- we can use the DataLoader, and pass through our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size = 50, shuffle = True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size = 50, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow us to more efficiently update our parameters in gradient descent, we use an Adam optimizer -- whatever that is -- passing through our networks parameters (the layers) and specifying a learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0005)\n",
    "x_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is the training process.  We go through the training data eight times.  And on each batch of data, we make predictions by passing our data through the neural network, and then calculating the loss and determining how to update the data by calculating the gradient, and then using that calculation to update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5310, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4799, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4760, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4926, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4911, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5090, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5029, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4888, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(8):\n",
    "    for X, y in trainset:\n",
    "        net.zero_grad()  \n",
    "        predictions = net(X.view(-1,28*28))  \n",
    "        loss = x_loss(predictions, y) \n",
    "        loss.backward()  \n",
    "        optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = testset.dataset.data.view(-1, 784).float()\n",
    "predictions_test = net(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
       "        4, 0, 7, 4, 0, 1, 3, 1, 3, 0, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2, 3, 5, 1, 2,\n",
       "        4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3, 7, 4, 6, 4, 3, 0, 7, 0,\n",
       "        2, 9, 1, 7, 3, 2, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3, 6, 1, 3, 6, 4, 3, 1, 4,\n",
       "        1, 7, 6, 9])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(predictions_test, axis = 1)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
       "        4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2, 3, 5, 1, 2,\n",
       "        4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3, 7, 4, 6, 4, 3, 0, 7, 0,\n",
       "        2, 9, 1, 7, 3, 2, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3, 6, 1, 3, 6, 9, 3, 1, 4,\n",
       "        1, 7, 6, 9])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.dataset.targets[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this does a good job of taking in data and predicting the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9629"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix\n",
    "\n",
    "accuracy_score(testset.dataset.targets, torch.argmax(predictions_test, axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Towards data science Pytorch Gradients](https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pytorch viz](https://github.com/szagoruyko/pytorchviz)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
