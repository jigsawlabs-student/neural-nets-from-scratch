{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and pass data through our layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(n_features, neur_l1, neur_out):\n",
    "    W1 = np.random.randn(n_features, neur_l1) / np.sqrt(n_features)\n",
    "    b1 = np.zeros((1, neur_l1))\n",
    "    W2 = np.random.randn(neur_l1, neur_out) / np.sqrt(neur_l1)\n",
    "    b2 = np.zeros((1, neur_out))\n",
    "    model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialized a set of weight matrices (`W1`, and `W2`) and bias vectors (`b1` and `b2`), which we later tune through our training procedure.  Once these weights and biases are trained, we can use them to make a prediction with our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With pytorch, we can initialize our weights and biases all at once through the Linear function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=8, bias=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 = nn.Linear(784, 8)\n",
    "W1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we created a linear layer that consists of eight neurons, each which takes in 784 features.  This Linear object consists of both a weight matrix and a bias vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0082, -0.0310, -0.0260,  ..., -0.0095, -0.0038,  0.0065],\n",
       "         [ 0.0193,  0.0050,  0.0018,  ...,  0.0122, -0.0049, -0.0053],\n",
       "         [-0.0328,  0.0264,  0.0198,  ..., -0.0072,  0.0268,  0.0058],\n",
       "         ...,\n",
       "         [ 0.0050, -0.0222,  0.0079,  ..., -0.0232, -0.0056, -0.0321],\n",
       "         [-0.0302, -0.0064, -0.0283,  ...,  0.0234, -0.0241, -0.0021],\n",
       "         [ 0.0050,  0.0298,  0.0083,  ...,  0.0335, -0.0152, -0.0197]]),\n",
       " tensor([ 0.0073,  0.0197,  0.0271, -0.0315,  0.0306,  0.0155,  0.0254, -0.0170]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.weight.data, W1.bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with a Pytorch Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we just saw that with Pytorch, we don't need to create a weight matrix and a bias vector individually, but rather we create both initializing a linear layer.  If we look closely, we can see that we are now not working with numpy arrays, but pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0082, -0.0310, -0.0260,  ..., -0.0095, -0.0038,  0.0065],\n",
       "         [ 0.0193,  0.0050,  0.0018,  ...,  0.0122, -0.0049, -0.0053],\n",
       "         [-0.0328,  0.0264,  0.0198,  ..., -0.0072,  0.0268,  0.0058],\n",
       "         ...,\n",
       "         [ 0.0050, -0.0222,  0.0079,  ..., -0.0232, -0.0056, -0.0321],\n",
       "         [-0.0302, -0.0064, -0.0283,  ...,  0.0234, -0.0241, -0.0021],\n",
       "         [ 0.0050,  0.0298,  0.0083,  ...,  0.0335, -0.0152, -0.0197]]),\n",
       " tensor([ 0.0073,  0.0197,  0.0271, -0.0315,  0.0306,  0.0155,  0.0254, -0.0170]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.weight.data, W1.bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll explore some of the differences between numpy arrays and pytorch tensors a bit later, but for now, it's enough to know that they work in similar ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we want to see the shape of our data, we simply use the shape function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 784])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.weight.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we want to select data, we do so using our same bracket notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0082, -0.0310, -0.0260],\n",
       "        [ 0.0193,  0.0050,  0.0018]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.weight.data[:2, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now most importantly for us, is that we can perform the same matrix operations that we have with numpy.  Let's see this next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working towards a Forward function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the point of initializing our weight and bias vectors is that they are the linear component to our neurons.  And each neuron ultimately produces an output.  \n",
    "\n",
    "In the input layer to a neural network, we take in a row of data, and pass our data through the linear function, or layer if we are talking about multiple neurons.  We do so by performing matrix vector multiplication.  In Pytorch, we do precisely the same thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's see this.  We'll begin by initializing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 784])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's select the weights of a single neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size = 10, shuffle = True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    W1 = nn.Linear(28*28, 64)\n",
    "    W2 = nn.Linear(64, 10)\n",
    "    return {'W1': W1, 'W2': W2} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Forward Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "X_1 = torch.rand(28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2319,  0.7662, -0.7318,  0.1258,  0.4845,  0.4858,  0.1362,  0.3198,\n",
       "        -0.0967, -0.3745, -0.1128, -0.2208,  0.1297, -0.1437, -0.4191,  0.1762,\n",
       "         0.1912,  0.3604, -0.4663, -0.0506, -0.3978, -0.0748, -0.3987, -0.2132,\n",
       "         0.1109,  0.3667,  0.8042,  0.3077,  0.0352, -0.1009,  0.1312,  0.2380,\n",
       "         0.1128,  0.0030,  0.1113, -0.5154, -0.3886,  0.0216,  0.6347, -0.7047,\n",
       "        -0.6546,  0.2967,  0.1723,  0.0944,  0.1739, -0.3365, -0.4871, -0.1153,\n",
       "         0.3490, -0.6262,  0.1084, -0.2347,  0.3372, -0.1943,  0.1036, -0.4603,\n",
       "        -0.5398, -0.1378,  0.3041,  0.2011, -0.7347,  0.3061, -0.3538,  0.3311],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1(X_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2319, -0.2023, -0.2146,  ..., -0.2262, -0.2268, -0.2174],\n",
       "        [ 0.7366,  0.7662,  0.7539,  ...,  0.7423,  0.7417,  0.7511],\n",
       "        [-0.7491, -0.7195, -0.7318,  ..., -0.7434, -0.7440, -0.7345],\n",
       "        ...,\n",
       "        [ 0.3004,  0.3300,  0.3177,  ...,  0.3061,  0.3055,  0.3149],\n",
       "        [-0.3589, -0.3293, -0.3415,  ..., -0.3532, -0.3538, -0.3443],\n",
       "        [ 0.3165,  0.3461,  0.3339,  ...,  0.3223,  0.3216,  0.3311]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1 = weights @ (X_1.view(1, -1).T) + W1.bias\n",
    "z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4423, 0.4496, 0.4466,  ..., 0.4437, 0.4435, 0.4459],\n",
       "        [0.6762, 0.6827, 0.6800,  ..., 0.6775, 0.6774, 0.6794],\n",
       "        [0.3210, 0.3275, 0.3248,  ..., 0.3223, 0.3221, 0.3242],\n",
       "        ...,\n",
       "        [0.5745, 0.5818, 0.5788,  ..., 0.5759, 0.5758, 0.5781],\n",
       "        [0.4112, 0.4184, 0.4154,  ..., 0.4126, 0.4125, 0.4148],\n",
       "        [0.5785, 0.5857, 0.5827,  ..., 0.5799, 0.5797, 0.5820]],\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Forward Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_numpy(X, model):\n",
    "    W1, b1, W2, b2 = tuple(model.values())\n",
    "    z1 = X.dot(W1) + b1 \n",
    "    a1 = sigma(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    return (z1, a1, z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, model):\n",
    "    W1, W2 = tuple(model.values())\n",
    "    Z1 = W1(X)\n",
    "    A1 = F.sigmoid(Z1)\n",
    "    Z2 = W2(A1)\n",
    "    A2 = F.softmax(Z2)\n",
    "    return (Z1, A1, Z2, A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeff/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "Z1, A1, Z2, A2 = forward(X_1.view(1, -1), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0982, 0.1197, 0.1461, 0.0512, 0.1394, 0.0906, 0.0712, 0.0601, 0.1250,\n",
       "         0.0986]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0341,  0.1631,  0.3626, -0.6857,  0.3155, -0.1150, -0.3567, -0.5252,\n",
       "          0.2067, -0.0308]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
