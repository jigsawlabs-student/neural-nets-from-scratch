{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Layers Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we'll reconstruct the hypothesis and forward functions for a neural network, all using Pytorch.  At the end we'll have functions that can initialize a neural network, and use the network to make predictions for our MNIST dataset.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_observation = X[:1]\n",
    "first_observation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see that this represents the number 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with a Linear Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's initialize our first linear layer.  Initialize a `Linear` layer with 64 neurons each of which take in the features of an observation from our MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "torch.manual_seed(123)\n",
    "\n",
    "W1 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.in_features\n",
    "\n",
    "# 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.out_features\n",
    "\n",
    "# 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the shape of the weight matrix and the bias vector initialized in our layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First look at the shape of the weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 784])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# torch.Size([8, 784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> And then let's return the shape of the bias vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# torch.Size([8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pass through some data through the linear layer.  To do so, we'll first have to translate our numpy array into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X_tensor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We need to convert the tensor to be of type float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_two_observations = X_tensor[:2].float()\n",
    "first_two_observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pass these two observations through the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2.5655,   44.3726,  -50.8704,  -31.6235,  -75.0541,  -56.1966,\n",
       "          -14.1126,  -45.3884],\n",
       "        [  86.7214,  -24.9196,   96.3119,  -63.9667,  -69.7478, -144.3202,\n",
       "           10.7071,  -31.4456]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1()\n",
    "\n",
    "# tensor([[   2.5655,   44.3726,  -50.8704,  -31.6235,  -75.0541,  -56.1966,\n",
    "#           -14.1126,  -45.3884],\n",
    "#         [  86.7214,  -24.9196,   96.3119,  -63.9667,  -69.7478, -144.3202,\n",
    "#            10.7071,  -31.4456]], grad_fn=<AddmmBackward>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have 8 outputs for each observation.  Now, reproduce the same numbers using matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2.5655,   44.3726,  -50.8704,  -31.6235,  -75.0541,  -56.1966,\n",
       "          -14.1126,  -45.3884],\n",
       "        [  86.7214,  -24.9196,   96.3119,  -63.9667,  -69.7478, -144.3202,\n",
       "           10.7071,  -31.4456]], grad_fn=<PermuteBackward>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# tensor([[   2.5655,   44.3726,  -50.8704,  -31.6235,  -75.0541,  -56.1966,\n",
    "#           -14.1126,  -45.3884],\n",
    "#         [  86.7214,  -24.9196,   96.3119,  -63.9667,  -69.7478, -144.3202,\n",
    "#            10.7071,  -31.4456]], grad_fn=<PermuteBackward>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, enough playing around.  Now it's time to initialize our linear layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Write a function called `init_model`.  The function should return a dictionary with keys of `W1` and `W2`, to represent our two layers.  The layers should take in data observations each with 784 features.  The first layer should have 64 neurons, and the second layer should return a vector of length 10 for each observation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def init_model():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': Linear(in_features=784, out_features=64, bias=True),\n",
       " 'W2': Linear(in_features=64, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = init_model()\n",
    "model\n",
    "# {'W1': Linear(in_features=784, out_features=64, bias=True),\n",
    "#  'W2': Linear(in_features=64, out_features=10, bias=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could take our data and pass it through these linear layers, but that would not be a valid neuron.  We need our activation layers.  We'll use two activation functions: the sigmoid function and the softmax function.\n",
    "\n",
    "> As we saw previously the softmax function can be used to return an output from our last layer.  The function exaggerates the preference from the linear layer, and returns a set of probabilities that add up to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.9724e-19, 1.0000e+00, 4.3300e-42, 9.8924e-34, 0.0000e+00, 2.1019e-44,\n",
       "         3.9829e-26, 1.0406e-39],\n",
       "        [6.8365e-05, 0.0000e+00, 9.9993e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         6.6412e-38, 0.0000e+00]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "predictions = None\n",
    "\n",
    "predictions\n",
    "\n",
    "# tensor([[6.9724e-19, 1.0000e+00, 4.3300e-42, 9.8924e-34, 0.0000e+00, 2.1019e-44,\n",
    "#          3.9829e-26, 1.0406e-39],\n",
    "#         [6.8365e-05, 0.0000e+00, 9.9993e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
    "#          6.6412e-38, 0.0000e+00]], grad_fn=<SoftmaxBackward>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see that the probabilities of each row add up to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.sum(axis = 1)\n",
    "\n",
    "# tensor([1.0000, 1.0000], grad_fn=<SumBackward1>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we should now be ready to write a `forward` function that takes in our data X, and returns a set of 10 observations using our two linear layers and activation layers of sigmoid and softmax. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "z_1 & = xW_1 + b_1 \\\\\n",
    "a_1 & = \\sigma(z_1) \\\\\n",
    "z_2 & = a_1W_2 + b_2 \\\\\n",
    "\\text{predictions} & = \\text{softmax(z_2)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, X):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0739, 0.0943, 0.0598, 0.0721, 0.1460, 0.0820, 0.2049, 0.0732, 0.1394,\n",
       "         0.0546],\n",
       "        [0.1017, 0.0406, 0.0614, 0.0767, 0.2207, 0.0680, 0.1502, 0.0989, 0.1457,\n",
       "         0.0360]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(model, first_two_observations)\n",
    "\n",
    "# tensor([[0.0739, 0.0943, 0.0598, 0.0721, 0.1460, 0.0820, 0.2049, 0.0732, 0.1394,\n",
    "#          0.0546],\n",
    "#         [0.1017, 0.0406, 0.0614, 0.0767, 0.2207, 0.0680, 0.1502, 0.0989, 0.1457,\n",
    "#          0.0360]], grad_fn=<SoftmaxBackward>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that, we've built the hypothesis function of a neural network using Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we saw how we can work with the Pytorch library to both initialize our linear layers and then write a `forward` function that takes in our data and returns a set of predictions for each observation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Towards data science Pytorch Gradients](https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pytorch viz](https://github.com/szagoruyko/pytorchviz)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
