{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pytorch Linear Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lesson, we learned about Pytorch tensors and saw how we can use them to perform many of the same operations that we have in numpy.  In constructing the weights and biases for a neural network, we can use tensors to create our weight matrices and bias vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "W1 = torch.randn(8, 15)\n",
    "b1 = torch.randn(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll see how we can use a Linear object to work with both a weight matrix and bias vector simultaneously. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we've initialized a linear layer of our neural network through creating a weight matrix and a bias vector.  But going forward, we'll use the Linear object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=15, out_features=8, bias=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "W1 = nn.Linear(15, 8)\n",
    "W1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we just initialized a linear layer that takes in 64 features, and outputs a vector of length 8 for each observation.\n",
    "\n",
    "This linear object contains both the weight matrix and bias vector that we saw before.  Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 0.0153, -0.2517,  0.0039,  0.2195,  0.0455, -0.0423,  0.1498, -0.0034,\n",
       "           0.1061, -0.0602,  0.0272, -0.1746, -0.2355,  0.2385,  0.1183],\n",
       "         [ 0.0416,  0.0378,  0.0023,  0.1865,  0.1587,  0.1989, -0.1091, -0.1594,\n",
       "           0.1042, -0.1967,  0.0451, -0.2548,  0.0320, -0.0221, -0.2207],\n",
       "         [-0.0980, -0.1517, -0.0681,  0.1166, -0.1324,  0.2280, -0.0969, -0.1797,\n",
       "           0.0771, -0.1517, -0.0594,  0.0892, -0.1748, -0.2192,  0.0030],\n",
       "         [-0.2560, -0.0397, -0.2001, -0.2446,  0.1498, -0.0142,  0.1464,  0.0254,\n",
       "           0.0066,  0.1346,  0.0430, -0.0201,  0.1950, -0.0395, -0.0901],\n",
       "         [ 0.1232,  0.2515, -0.2305,  0.0939, -0.2218,  0.0135, -0.2104,  0.2278,\n",
       "          -0.2412, -0.2580,  0.0965,  0.0917,  0.0793, -0.0584,  0.1782],\n",
       "         [ 0.1335,  0.0241, -0.0353, -0.0671,  0.1532, -0.2241, -0.2580, -0.0652,\n",
       "           0.1999, -0.1292,  0.2364,  0.1334, -0.0924, -0.0818, -0.1487],\n",
       "         [ 0.0585, -0.1632,  0.1573,  0.1465, -0.1868,  0.1669,  0.1928,  0.1211,\n",
       "          -0.1521,  0.2563,  0.2176, -0.2566, -0.2298, -0.1749, -0.1267],\n",
       "         [ 0.1293, -0.2004, -0.0440,  0.1219,  0.0255, -0.2175,  0.2479,  0.0458,\n",
       "           0.1488, -0.1809,  0.2237, -0.0704, -0.0578,  0.1507,  0.1704]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2310, -0.2375, -0.1439, -0.0767,  0.1260,  0.0337, -0.2457,  0.0590],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.weight, W1.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we want to pass through an observation through these weight and matix vectors, we can do so with the matrix operations we saw previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1694,  2.0995, -1.4558,  0.2007,  0.5987, -0.4140, -1.5270,  1.2951,\n",
       "         -0.5993,  0.0120, -1.4823,  1.6493, -0.7202,  1.8320,  1.3714]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1 = torch.randn(1, 15)\n",
    "X_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 15]), torch.Size([15, 1]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.weight.shape, X_1.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3203],\n",
       "        [-0.8808],\n",
       "        [-0.4242],\n",
       "        [-0.0724],\n",
       "        [ 1.4462],\n",
       "        [-0.1124],\n",
       "        [-1.9090],\n",
       "        [-0.6849]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.weight @ X_1.T\n",
    "#  + W1.bias.view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can just pass our data directly through our linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1694,  2.0995, -1.4558,  0.2007,  0.5987, -0.4140, -1.5270,  1.2951,\n",
       "         -0.5993,  0.0120, -1.4823,  1.6493, -0.7202,  1.8320,  1.3714]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5513, -1.1182, -0.5681, -0.1491,  1.5722, -0.0787, -2.1548, -0.6259]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z1 = W1(X_1)\n",
    "Z1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see, this performs the calculations of our linear layer for us -- both the matrix multiplication and the addition of the bias vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completing the Activation Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So at this point we know how to both initialize and pass data through a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=15, out_features=8, bias=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 = nn.Linear(15, 8)\n",
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5759,  0.7659, -0.0972,  0.0187,  0.9738, -0.1467,  0.2360, -0.0631]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z1 = W1(X_1)\n",
    "\n",
    "Z1.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about an activation layer.  Well remember the activation layer simply means that we pass our linear outputs through a non-linear function like the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1714, 0.6826, 0.4757, 0.5047, 0.7259, 0.4634, 0.5587, 0.4842]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "sigmoid(Z1.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, Pytorch has a built in function for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1714, 0.6826, 0.4757, 0.5047, 0.7259, 0.4634, 0.5587, 0.4842]],\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(Z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we learned how to construct our linear and activation layers in Pytorch.  We saw that we can initialize a linear layer through the Linear constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "\n",
    "W1 = Linear(15, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that our linear layer consists of both a weight matrix and a bias vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 15]), torch.Size([8]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.weight.shape, W1.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that to multiply our data by our weight matrix and then add our bias vector, we simply pass our data through the instance of the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2126,  0.0518,  0.6054, -0.0200,  0.4632,  0.4092,  0.1312,  0.4435]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1 = torch.randn(1, 15)\n",
    "W1(X_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then moved onto the activation layer, which is simply a non-linear function.  If we use a sigmoid function as our activation layer, we can access this from Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1714, 0.6826, 0.4757, 0.5047, 0.7259, 0.4634, 0.5587, 0.4842]],\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(Z1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
