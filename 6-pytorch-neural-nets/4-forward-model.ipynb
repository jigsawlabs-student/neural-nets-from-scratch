{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Oriented Pytorch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lessons, we became familiar with tensors, linear layers, and activation functions in Pytorch to initialize a model and perform a forward pass of the data.  So far, things have been pretty similar to what we have seen in numpy.  We initialize some sample numbers for our weights and biases, and use functions as our activation layers.  \n",
    "\n",
    "We did this through two functions: an `init_model` function and a `forward` function.  Because these two functions relate to the same neural network, we might imagine combining them into a single class.  That is what we'll accomplish in this lesson.\n",
    "\n",
    "Later on, we'll see that combining these functions in a class, allows Pytorch to easily calculate our gradients when we eventually train our neural network.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Functions to Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have written two functions to initialize a model and pass through our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': Linear(in_features=784, out_features=64, bias=True),\n",
       " 'W2': Linear(in_features=64, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "def init_model():\n",
    "    W1 = nn.Linear(28*28, 64)\n",
    "    W2 = nn.Linear(64, 10)\n",
    "    return {'W1': W1, 'W2': W2} \n",
    "\n",
    "model = init_model()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, model):\n",
    "    W1, W2 = tuple(model.values())\n",
    "    Z1 = W1(X)\n",
    "    A1 = F.sigmoid(Z1)\n",
    "    Z2 = W2(A1)\n",
    "    A2 = F.softmax(Z2, dim = 1)\n",
    "    return (Z1, A1, Z2, A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now moving these two functions into a class really doesn't change our two functions too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(28*28, 64)\n",
    "        self.W2 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        A1 = torch.sigmoid(self.W1(x))\n",
    "        return F.softmax(self.W2(A1), dim = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main changes here.  First, we move our init_model function into the `__init__` function.  As we know from object orientation, this function will now be called when we initialize our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (W1): Linear(in_features=784, out_features=64, bias=True)\n",
       "  (W2): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ??nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we have our class inherit from our `nn.Module` function.  This adds functionality to our class, which we'll see later.\n",
    "\n",
    "Finally, we set up call the `__init__` function in the `nn.Module` class, through the `super().__init__()` init line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it.  We've successfully setup a neural network in Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Use of Our Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is the code we needed to translate our functions to a class in Pytorch.  Now let's see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        A1 = torch.sigmoid(self.fc1(x))\n",
    "        return F.softmax(self.fc2(A1), dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to initialize our neural network, and assign it to a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's pass some data through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First we initialize an instance with one row, 784 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.randn(1, 28*28)\n",
    "# x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we pass through our data to our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0882, 0.0702, 0.1430, 0.0827, 0.0477, 0.1715, 0.1099, 0.1002, 0.1011,\n",
       "         0.0855]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that passing through the data automatically calls the `forward` function in our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    A1 = torch.sigmoid(self.fc1(x))\n",
    "    return F.softmax(self.fc2(A1), dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that we've completed our hypothesis function in Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Towards data science Pytorch Gradients](https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pytorch viz](https://github.com/szagoruyko/pytorchviz)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
