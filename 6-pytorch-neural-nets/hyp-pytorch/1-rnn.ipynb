{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hidden states\n",
    "    * To compute the first hidden state, compute based on previous hidden state and current input\n",
    "    * Initial hidden state is h_0, and can learn how to initialize (or zero vector).\n",
    "    \n",
    "    * And to compute the new one \n",
    "        * Do linear transformation on previous hidden state, and on the current input, and then add a bias\n",
    "        * $h(t) = \\sigma(W_h h^{t - 1} + W_\\epsilon e^{(t)} + b_1)$\n",
    "            * This produces the new hidden state\n",
    "            \n",
    "* And then compute the next hidden state, and so on. \n",
    "* So then to predict next word, can take the current hidden state, with a linear layer (with softmax) to get output distribution (predicting word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* And we'll have to learn both $W_e$ and $W_h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Advantages\n",
    "    * Can process any length input\n",
    "    * Computation for step t can (in theory) use information from many steps back.\n",
    "    * Model size does not increase for longer input\n",
    "    (Size of the model fixed (W_h and W_e biases).  And do not get bigger with longer inputs, because just apply the same weights repeatedly\n",
    "    * Apply same weights on every time steps, (same transformation to all inputs) so can share learnings from previous inputs\n",
    "    \n",
    "2. Disadvantages\n",
    "    * Recurrent computation is **slow**\n",
    "        * Need to compute hidden state based on previous hidden state (so over a long sequence)\n",
    "        * In practice, difficult to access information from many steps back (exploding/vanishing gradients)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How can you apply this same weight matrix to all of these different words?\n",
    "    * Idea is to learn a general function (not just how to deal with a specific word), given the words so far.  How to deal. with language given context so far\n",
    "    * And all of words are vectors of length 500\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation for RNNs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What's the derivative of $J^t(\\theta)$ wrt the repeated weight matrix?\n",
    "\n",
    "The gradient with respect to a repeated weight is the sum of the gradient wrt each time it appears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./backprop.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta J^{(t)} }{\\delta W_h} = \\sum_{i = 1}^t \\frac{\\delta J ^{(t)}}{\\delta W_h}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backprop through time (same as backprop before (just for rnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of batch\n",
    "\n",
    "* The size of the batch matters.  True.  And this is why stochastic gradient descent is only an approximation of true gradient descent.  Because the gradient you compute is just an approximation of true gradient of (so why shuffling data is important).  \n",
    "\n",
    "* But over many steps should minimize your loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RNN for sentence classification (eg sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Encode the text using the rnn. So then have some kind of sentence encoding, so that can output the label for the sentence.  Would be useful to have a single vector to represent the sentence rather than lot of separate vectors.\n",
    "\n",
    "* So we can use the final hidden state as the sentence encoding.  Remember the final hidden state predicts what comes next.  So we presume it knows about everything that came so far.\n",
    "\n",
    "* But better is to take an elementwise max or mean of all hidden states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs as general encoder module\n",
    "\n",
    "* Question answering: \n",
    "    * What nationality was beethoven?\n",
    "    1.  Use RNN to process the question.\n",
    "        * Use the hidden states as representation of the question\n",
    "        * So the hidden states represent the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RNN Medium Pytorch](https://towardsdatascience.com/sentiment-analysis-using-lstm-step-by-step-50d074f09948)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[multivariable chain rule khan academy](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/differentiating-vector-valued-functions/a/multivariable-chain-rule-simple-version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Stanford RNN](https://www.youtube.com/watch?v=6niqTuYFZLQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Stanford nlp rnn](https://youtu.be/iWea12EAu6U?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&t=953)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Colab RNN](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb)\n",
    "\n",
    "[Andrej Code](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jeremy Howard RNN](https://www.youtube.com/watch?v=H3g26EVADgY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Sentiment Pytorch RNN Code](https://github.com/scoutbee/pytorch-nlp-notebooks/blob/develop/3_rnn_text_classification.ipynb)\n",
    "\n",
    "[Pytorch NLP Videos](https://github.com/scoutbee/pytorch-nlp-notebooks)\n",
    "\n",
    "[Olah LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tabular dataset](https://averdones.github.io/reading-tabular-data-with-pytorch-and-training-a-multilayer-perceptron/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
