{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perspectives on a Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lessons we built up to our sigmoid neuron.  We saw that a neuron was a linear function wrapped in a non-linear activation function.  In this lesson, let's see a couple of ways of representing this with a summation notation and with a dot product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that our sigmoid neuron consists of a linearity and an activation function where our activation function is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z(f(x)) =  \\frac{1}{1 + e^{-f(x)}} $\n",
    "\n",
    "and our linear function is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) = w_1x_1 + w_2x_2 ... w_nx_n + b $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./neuron-math.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the summation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the work of rewriting our linear function.  If we look at our linear function, we see that almost all of it is list of similar terms added together $w_ix_i$ plus our bias term, $b$.  Here's how we can rewrite this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) = w_1x_1 + w_2x_2 ... w_nx_n + b  = \\sum_{i = 1}^n w_ix_i + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the summation symbol is essentially a loop in Python.  It tells us to iterate through each element term in the linear equation, and then add the bias, $b$.  This is how we could express this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [3, 2]\n",
    "\n",
    "def foody_bar_perceptron(features):\n",
    "    bias = 25\n",
    "    summation = 0\n",
    "    weights = [2, 3]\n",
    "    for idx in range(0, len(features)):\n",
    "        summation +=  weights[idx]*evidence[idx]\n",
    "    return summation + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) = \\sum_{i = 1}^n w_ix_i + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our summation above expresses the same concept.  And the $i=1$ on the bottom says to begin at the index of 1, and go until a number $n$ -- where $n$ is the number of $w_ix_i$ terms in our equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another rewrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go back to representing our linearity function.  Currently we have something that looks like the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) = \\sum_{i = 1}^n w_ix_i + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's focus on the $\\sum_{i = 1}^n w_ix_i$ component.  Let's work towards rewriting this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the summation represents that there is a list of weights, $w_i$ and that there is a list of related observations $x_i$.  Like the sweetness taste and sweetness smell inputted to a neuron, $x_i$ and $x_2$, and the weighing of these components $w_1$ and $w_2$.  Let's place these into two lists, $x$ and $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [2, 4]\n",
    "# sweet smell is 2\n",
    "# sweet taste is 4\n",
    "\n",
    "w = [1, 3]\n",
    "# weigh smell 1\n",
    "# weigh taste 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in math terms, we call these two lists vectors, $w$, and $x$.  And the *dot product* of these two vectors, is pairing the elements of these vectors together by their index, multiplying the pairs, and then adding.  It looks like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "2 & 4 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "1 \\\\\n",
    "3 \n",
    "\\end{bmatrix} = 2*1 + 4*3 = 14$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([2, 4])\n",
    "w = np.array([1, 3])\n",
    "b = -12\n",
    "x.dot(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so we can rewrite our linear function as: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $f(x) = w_1x_1 + w_2x_2 ... w_nx_n + b $\n",
    "* $= \\sum_{i = 1}^n w_ix_i + b $\n",
    "* $= w \\cdot x + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this allows us to rewrite our sigmid neuron as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8807970779778823"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(value):\n",
    "    return 1/(1 + np.exp(-value))\n",
    "\n",
    "sigmoid(x.dot(w) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we can represent the sigmoid function by the greek letter $\\sigma$ (sigma), we can express our entire sigmoid neuron as: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma(x \\cdot w + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's it, our linear function wrapped in our sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
