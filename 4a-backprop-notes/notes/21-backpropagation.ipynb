{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, with neural networks we a set of inputs, followed by a layer of linear functions and activation functions, and ultimately our classifier (like a softmax function).\n",
    "\n",
    "This is our hypothesis function.  In this lesson we'll see how we train this neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward then Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of training a neural network is similar to that of training of any other machine learning algorithm.  We use a prediction function to predict against our training data, and then we correct.\n",
    "\n",
    "With something like a logistic regression function, this means that we change our parameters to minimize a cost function.  With a neural network, it's the same thing.  \n",
    "\n",
    "As we saw in logistic regression, we can look to gradient descent to see how to update our parameters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, we see how much a change in any of the parameters affects the cost function, and our algorithm alters the parameters proportionate to their impact in reducing our cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now because our neural network consists of layers functions, with one layer dependent on the one that preceded it, discovering the impact of changing one of the entries in our weight grid can appear a little more complicated than with simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./multilayer.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all how can our algorithm know how much to alter a parameter back in layer two?  To do so, it would need to calculate the impact that this parameter would have down the chain in making a hypothesis, and through this, in impacting the cost function.  So how do we calculate this?\n",
    "\n",
    "The key, perhaps is to rewrite our network as a series of functions?\n",
    "\n",
    "$J(h(L_4(L_3(L_2\\theta(x)))))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can rewrite this question as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta J}{\\delta \\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is we want to discover the change in the cost as we change the parameters of layer 2.  And we *can* in fact discover this, it's just the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A concrete example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think about what our neural network looks like in practice.  Generally, our formula is something like the following: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J = (y - \\hat{y})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write the output of the final layer as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a_{last} = \\sigma(x*w + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our cost formula is really something like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J = (y - \\hat{y})^2 = (y - \\sigma(x*w + b))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the value $x$ is our inputs to the layer.  For the first layer, this would just be our data.  But for any other layer, these inputs come from the output of the previous layer.  So we can say that the output of any layer is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma(a^{L - 1}*w + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $a^{L-1}$ is the output of the previous layer.  So we can rewrite our cost formula as: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J = (y - \\sigma(a^{L -1}*w + b))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisualizing our problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we think about changing our weights, we could visualize it as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./chained.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where we think about changing the parameters of a layer, and this trickles down to influencing our cost function.  Or we could also see this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost():\n",
    "    actual = 1\n",
    "    return (actual - prediction())**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction():\n",
    "    return 1/(1 + np.exp(z()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z():\n",
    "    a_l_1, a_l_2 = 1, 2\n",
    "    w_1, w_2, b = 3, 4, 5\n",
    "    return a_l_1*w_1 + a_l_2*w_2 + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we calculate the gradient, the goal is to see how much a parameter like `w_1` would effect our overall cost function.  The way of calculating this mathematically is to ask, what is $\\frac{\\delta C}{\\delta w}$.  How does C change as we nudge w?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$C(a(z)))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate this, we can break this problem up as answering how much does:\n",
    "\n",
    "1. z change as w changes\n",
    "2. a^L change as z changes\n",
    "3. And C change as a^L changes.\n",
    "\n",
    "So $\\frac{\\delta C}{\\delta w} = \\frac{\\delta z}{\\delta w}\\frac{\\delta A^L}{\\delta z}\\frac{\\delta C}{\\delta A^L}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./chained.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we saw that to calculate C, we simply apply the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta C}{\\delta w} = \\frac{\\delta z}{\\delta w}\\frac{\\delta A^L}{\\delta z}\\frac{\\delta C}{\\delta A^L}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how to solve this given our respective functions of $z$, $A^L$, and $C$, which are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $z = (a^{L -1}*w + b)$\n",
    "* $a^L = \\sigma(z)$\n",
    "* $C = (y - a^L)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./backprop-slide.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So $\\frac{\\delta z}{\\delta w} = a^{L - 1}$\n",
    "\n",
    "$\\frac{\\delta A^L}{\\delta z} = \\sigma'(z)$\n",
    "\n",
    "$\\frac{\\delta C}{\\delta A^L} = 2(y - a^L)$\n",
    "\n",
    "or \n",
    "\n",
    "$ \\frac{\\delta C}{\\delta w} = a^{L - 1}*\\sigma'(z)*2(y - a^L)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Backpropagation calculus - 3blue1brown](https://www.youtube.com/watch?v=tIeHLnjs5U8)\n",
    "\n",
    "[backprop](https://www.youtube.com/watch?v=ZIYOSdioBCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Talk about how start off with random numbers, and then backwards propagate\n",
    "* Change to removing the sigmoid function to make things simpler\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
