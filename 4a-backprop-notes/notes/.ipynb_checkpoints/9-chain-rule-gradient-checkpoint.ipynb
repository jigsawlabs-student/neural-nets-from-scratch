{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we would probably like to go directly into using gradient descent to find the weights and biases of a neuron that minimize our cost curve. But doing so is not so simple. \n",
    "\n",
    "Remember that with gradient descent, we find the change in our function's output as we alter each parameter, and then step in in the steepest descent.  The issue with applying this technique is that the impact on the output is more indirect.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing the issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous lessons, we found the rate of change in our output as we changed the parameters in a function like:\n",
    "\n",
    "$$z(w,b) = 3w + b$$\n",
    "\n",
    "To find how the output of $f$ changes as we change each parameter, we take the partial derivative with respect to each parameter, above $w$ and $b$.  But when trying to find the parameters that minimize a cost curve, this time we are not finding the impact of altering the parameters on that same function $f$, but on another function, our cost function: \n",
    "\n",
    "$$J(w,b) = (y - z(w, b))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For right now, we are leaving off the summation, just to keep our function a little less intimidating.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we said this is more indirect.  And really it's more indirect than even that, because $z(w, b)$ is just the linear component, which is then fed to the activation function $\\sigma$ to make the prediction, which is then passed into our cost function:\n",
    "\n",
    "$$J(w,b) = (y - \\sigma(z(w, b)))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we said, seeing how the changing the parameters $w$ and $b$ impact our cost curve is more indirect than we previously saw.  \n",
    "\n",
    "But don't worry, mathematicians have already figured out how to solve problems like the one above.  We just have to learn their approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing the Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem that we are running into above, is how to find the derivative of a nested function.  Our linear function is passed to our sigmoid function, whose output is then passed to the cost function.\n",
    "\n",
    "> *Look to wikipedia to see how can reword nested functions*\n",
    "\n",
    "Let's see how we can find the derivative of nested functions with a simpler example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x) = (3x + 1)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Break it down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's turn this into code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x): \n",
    "    return (3*x + 1)**2\n",
    "\n",
    "f(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step to finding the derivative, is to break the function above into two functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x):\n",
    "    return 3*x + 1\n",
    "\n",
    "def g(y):\n",
    "    return y**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g(h(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can even redefine our function $f(x)$ so be composed of $h$ and $g$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return g(h(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see how that worked?  Let's see it again, this time with math.  We started with the function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x) = (3x + 1)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we broke this into two functions, $h(x)$ and $g(y)$ where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h(x) = 3x + 1$$\n",
    "$$ g(y) = y^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can rewrite our function, $f(x)$ as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x) = g(h(x)) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we broke our function $f(x)$ down above, by defining two functions $h(x)$ and $g(y)$, and then passing the output of $h(x)$ into $g(y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Finding the derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have rewritten our function \n",
    "$$f(x) = (3x + 1)^2$$ as:\n",
    "\n",
    "$$f(x) = g(h(x)) $$  where:\n",
    "\n",
    "$$h(x) = 3x + 1$$\n",
    "$$ g(h) = h^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the derivative $f(x)$ with respect to $x$ we do the following:\n",
    "\n",
    "> take the derivative of the our function outer $g(h)$ and multiply it by the derivative of the inner function $h(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f'(x) = g'(h(x)) * h'(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or using our other notation: $\\frac{\\delta f}{\\delta x} = \\frac{\\delta g}{\\delta h}*\\frac{\\delta h}{\\delta x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's solve these individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g(h(x)) =  h(x)^2$ and $\\frac{\\delta g}{\\delta h} =  2h(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h(x) =  3x + 1$, and $\\frac{\\delta h}{\\delta x} =  3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plug these components into $\\frac{\\delta f}{\\delta x} = \\frac{\\delta g}{\\delta h}*\\frac{\\delta h}{\\delta x}$.\n",
    "\n",
    "Substituting we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta f}{\\delta x} = \\frac{\\delta g}{\\delta h}*\\frac{\\delta h}{\\delta x} = 2h(x)*3 = 6*h(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And because $h(x) = (3x + 1)$, substituting further we get: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta f}{\\delta x} = 6h(x) = 6(3x + 1) = 18x + 6 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we just did is pretty cool.  We were able to see how calculate how nudging the value of $x$ impacts our function $$f(x) = (3x + 1)^2$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what we found was that: \n",
    "\n",
    "$\\frac{\\delta f}{\\delta x} = 18x + 6 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for example, when $x = 3$, the rate of change of our function with respect to $x$ is $18*3 + 4 = 60$.  And we can see this with our old formula of the derivative being $\\frac{\\delta y}{\\delta x} = lim_{\\delta x\\to0}\\frac{y_1 - y_0}{x_1 - x_0}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.90000000000012"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(f(3.1) - f(3))/(3.1 - 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The chain rule with the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $g(x) =  w_1x_1 + w_2x_2 + w_3x_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll continue to skip the sigmoid function.  Instead let's just assume that directly this input predicts whether or a cell is cancerous or not.  And the loss is.  So $g(x)$ is directly our hypothesis function, and our loss function is still directly:\n",
    "\n",
    "* $J(\\theta) = (actual - expected)^2$ or to place it another way\n",
    "\n",
    "* $J(\\theta) = \\sum (y_i - g(x_i))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the derivative we can get rid of ignore the summation, it won't make any difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $J(\\theta) = (y - g(x))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And remember that $g(x)$ is our weighted input, so let's plug that back in:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\theta) = (y - (w_1x_1 + w_2x_2 + w_3x_3 + w_0) )^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remember we want to find how we can update each of our parameters $w$ such that we start at a random set of parameters $w$, and then descend along our cost curve $J$.  And to do so, we move in the direction of the negative gradient.  Doing so will point us in the direction that descends along our cost curve.\n",
    "\n",
    "So to move in the direction of the negative gradient, we need to find the partial derivative with respect to each parameter $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta J}{\\delta w_1} = 2*(y - (w_1x_1 + w_2x_2 + w_3x_3 + w_0))*-x_1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta J}{\\delta w_2} = 2*(y - (w_1x_1 + w_2x_2 + w_3x_3 + w_0))*-x_2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And because it's the negative gradient, we reverse the signs of the partial derivatives to get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta J}{\\delta w_1} = 2(y - (w_1x_1 + w_2x_2 + w_3x_3 + w_0))*x_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta J}{\\delta w_2} = 2*(y - (w_1x_1 + w_2x_2 + w_3x_3 + w_0))*x_2 $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
