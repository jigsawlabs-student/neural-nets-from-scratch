{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In building and executing the hypothesis function, we were able to see we can build layers of a neural network, and connect them to make predictions.  But we would still like better understand how these these layers can make predictions that match our training data.  In other words, if we better understand the hypothesis function of a neural network, what about the training procedure of a neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that a neural network has so far looked like the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X -> lin1 -> nonlinear -> lin2 -> nonlinear -> predict_y -> mse`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remember that when we train a neural network, the question is, how do we alter the parameters in each of these layers so that they reduce the cost function.  \n",
    "\n",
    "We should most alter the parameters that have causes the largest change in the cost function.  Or in other words, we would like to calculate the derivative with respect to each parameter that we can alter.  That is, we would like to calculate the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The impact of the second linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, our neural network takes the following form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X -> lin1 -> nonlinear -> lin2 -> nonlinear -> predict_y -> mse`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can build these two layers linear layers through two different weight matrix.  Our task, when we train, is to find the changes to our weight matrices that will ultimately lead two predictions that minimize our mean squared error.  Now what makes this tricky is that the effect of our weight matrix, lin2, is indirect.  That is, the change in the weight matrix changes the output of the nonlinear component, which changes the predictions of y, which changes the mean squared error.\n",
    "\n",
    "`lin2 -> nonlinear -> predict_y -> mse`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is why we must apply the chain rule.  Applying the chain rule tells us that:\n",
    "\n",
    "$$\\frac{\\delta C}{\\delta L2_\\theta} = \\frac{\\delta L2}{\\delta L2_\\theta}* \\frac{\\delta R}{\\delta R_\\theta} * \\frac{\\delta MSE}{\\delta \\hat{y}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, the change in the cost function as we change the weights in our second layer equals the change in the output of second layer, multiplied by the change in our non-linear function multiplied by the change in our cost function.  So now it's our task to calculate each of these components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting from the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we just consider the last layer, the question would be how does altering the output `predict_y` decrease the cost of the function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`predict_y -> mse`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating this follows the rules of our rules of derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$MSE(\\theta) = (y - \\hat{y})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta C}{\\delta \\hat{y}} = 2(y - \\hat{y})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_grad(pred, target):\n",
    "    # grad of loss wrt previous layer (predictions)     \n",
    "    return 2. *(pred.squeeze() - target).unsqueeze(-1) / pred.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving backwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$relu_\\theta -> predict\\_y -> mse$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now the question what is the impact of when the output of the relu function changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mse(predict_y(relu(...)))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta C}{\\delta R_\\theta} = \\frac{\\delta R}{\\delta R_\\theta} * \\frac{\\delta C}{\\delta \\hat{y}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_grad(l_previous, grad_mse):\n",
    "    #  so change will be proportional * grad_mse   \n",
    "    return (l_previous>0).float() * grad_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So notice that in non-linear gradient there is actually nothing to modify\n",
    "* It's just to see the impact of the previous layer (So maybe should start at L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep going backwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ lin\\_2_\\theta -> relu -> predict\\_y -> mse$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mse(predict_y(relu(lin_2())))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x \\cdot W + b = \\begin{bmatrix}\n",
    " -6& 9 &3  &-10\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "3 &4 \\\\ \n",
    " 2&5 \\\\ \n",
    " 10& 6\\\\ \n",
    "6 & 1\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "10 \\\\ \n",
    " 2 \\\\ \n",
    "\\end{bmatrix} = \\begin{bmatrix} -20 \\\\ 31 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_linear(X, W, b):\n",
    "    return (X@W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are three components that can change the output.  \n",
    "* the input value of X (here the previous layer)\n",
    "* the weights of the matrix W \n",
    "* or the bias vector b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our critical task is to calculate the impact when of changing either of these parameters.  Once again we do this by calculating our gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating our gradient we have: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta C}{\\delta L2_\\theta} = \\frac{\\delta L2}{\\delta L2_\\theta} * \\frac{\\delta R}{\\delta R_\\theta} * \\frac{\\delta C}{\\delta \\hat{y}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we we think back to the execution of the linear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(X, prev_grad, w, b):\n",
    "    inp.g = out.g @ w.t()\n",
    "    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n",
    "    b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gradient of a matrix product is the matrix product with the transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(inp, targ):\n",
    "    l1 = inp @w1 + b \n",
    "    l2 = relu(l1)\n",
    "    output = l2 @ w2 + b2\n",
    "    \n",
    "    # backward pass\n",
    "    mse_grad(output, targ)\n",
    "\n",
    "    lin_grad(l2, out, w2, b2)\n",
    "    relu_grad(l1, )\n",
    "    lin_grad(inp, l1, w1, b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So $\\frac{\\delta z}{\\delta w} = a^{L - 1}$\n",
    "\n",
    "$\\frac{\\delta A^L}{\\delta z} = \\sigma'(z)$\n",
    "\n",
    "$\\frac{\\delta C}{\\delta A^L} = 2(y - a^L)$\n",
    "\n",
    "or \n",
    "\n",
    "$ \\frac{\\delta C}{\\delta w} = a^{L - 1}*\\sigma'(z)*2(y - a^L)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have implemented calculating the gradient for each layer, the next step is to progressively update our weight matrices by our gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches():\n",
    "    batches = []\n",
    "    # n -1 is size of the training set\n",
    "    # maybe there is a way to just partition the training set, the iterate through     \n",
    "    for i in range((n-1)//bs + 1):\n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        batches.append((xb, yb))\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .5 # Set a learning rate.\n",
    "epochs = 1 # how many cycles to train for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for xb, yb in get_batches():\n",
    "        hypothesis = model(xb)\n",
    "        loss = loss_func(hypothesis, yb)\n",
    "        \n",
    "        # this is the mse.loss, really gradient\n",
    "        # because needs to set the gradient for the chain rule         \n",
    "        loss.backward()\n",
    "        # When call loss.backward(), this recalculates all of the gradients for each layer\n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if has_attr(l, 'weight'):\n",
    "                    # go through each layer\n",
    "                    # find the gradient (but does weight.grad find it?)\n",
    "                    # and decrease by learning rate                     \n",
    "                    l.weight -= l.weight.grad * lr\n",
    "                    # this gradient is how much each weight it multiplied by, \n",
    "                    # so decrease each weight by the amount it is impacted                          \n",
    "                    l.bias -= l.bias.grad * lr\n",
    "                    \n",
    "                    # zero the gradients because \n",
    "                    # loss function recalculates the gradients                     \n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dl1/lesson2-sgd SGD - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 46:15 https://www.youtube.com/watch?v=AcA8HAYh7IE&t=888s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[fast_ai notebook](https://github.com/fastai/course-v3/blob/master/nbs/dl2/03_minibatch_training.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[michael nielsen](http://neuralnetworksanddeeplearning.com/chap4.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Look at building a neural network from scratch - especially diy with python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
