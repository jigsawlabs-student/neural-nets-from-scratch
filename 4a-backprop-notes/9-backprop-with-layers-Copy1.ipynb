{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  BackPropagation With Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the last lessons, we learned how to write a hypothesis function of our neural network that both consists of multiple "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why the chain rule is so important is because our neural network is one large composite function.  And if we want to see how to update the parameters of this composite function, then we need consider how each weight and bias ultimately effects the predictions we output from our neural network, and thus our cost function.  Let's see this, and then we'll use the chain rule to update the parameters of our function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks as a Composite Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that in our fully connected neural network, we have a series of layers, and ultimately output a prediction $\\hat{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "z_1 & = xW_1 + b_1 \\\\\n",
    "a_1 & = sigmoid(z_1) \\\\\n",
    "z_2 & = a_1W_2 + b_2 \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here we leave off the softmax function, to keep things simpler.  If interested, the resources below offer explanation as to how to include it in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can rewrite this as a composite function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y} = z_2(\\sigma(z_1(x)))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So interpreting the composite function above, we take the features of an observation, $x$, and feed that to our linear layer, whose output is fed to the sigmoid function, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So above we saw that we evaluate our neural network, with the cost function:\n",
    "\n",
    "$J(\\theta) = (y_i - z_2(\\sigma(z_1(x))))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We use $\\theta$ to broadly refer to the parameters of a function.  So here, $\\theta$ represents the weights and biases of both linear layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's say we want to make an update to the parameters in our neural network.  Let's just focus on the parameters in our second linear layer, $z_2$.  How should we update the parameters in $z_2$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well we should find the direction of steepest descent by finding how nudging the parameters of $z_2$ change the output of the cost function.  And we do this through the chain rule.\n",
    "\n",
    "The parameters of the second linear layer impact the cost function by changing the output of $z_2$, which changes the output of our cost function, $J$.  So:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta J}{\\delta{\\theta_{z_2}}} = \\frac{\\delta J}{\\delta{z_2}} * \\frac{\\delta z_2}{\\delta{\\theta}_{Z_2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we want to see how to update the parameters of $z_1$ to approach the minimum of the cost curve, we again, need use the chain rule to consider the impact of the parameters of $z_1$ on the rest of the neural network, and ultimately the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to do this, let's keep our procedure chain rule procedure of calculating each of our component derivatives individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving to Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now below we'll move through finding the gradient of some of the components of our neural network.  We do so to see the application of the chain rule in a neural network.  Our focus **will not** be on say finding the [derivative of the sigmoid function](https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e).  If you'd like a deeper discussion of these individual derivatives, check out the resources below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, again this is our cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\theta) = (y_i - z_2(\\sigma(z_1(x))))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Derivative of Cost Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first component that we can find the derivative of is the cost function.  Our cost function depends on the inputs from $z_2$.  And thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(z_2) = (y - z_2)^2$, and derivative of $J$ with respect to $z$ is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{dJ}{dz_2} = 2(y - z_2)*-1 = -2y + 2z_2 = 2(z_2 - y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Derivative of a Linear layer\n",
    "\n",
    "After finding the change in cost with respect to the outputs of $z_2$, we move backwards to calculating the derivative for the parameters of $z_2$.  How will our cost function change as we change these components?\n",
    "\n",
    "$$J(z_2(x,W,b))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we our second linear layer $z_2$ looks like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z_2 = xW_2 + b_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, $x$ is the output from our activation layer $\\sigma$, which we'll refer to as $a_1$.  So:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z_2 = a_1W_2 + b_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll need to use the derivative to find the impact of changing our parameters in $W$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To do so, we need to go into matrix calculus.  We won't prove it here, but [the general rule](https://explained.ai/matrix-calculus/) is, if $f(x) = x\\cdot W$, then \n",
    "> $\\frac{\\delta f}{\\delta x} = W^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So knowing this, we take the derivative of the parameters of $z_2 = a_1W_2 + b_2$ and get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta J}{\\delta W_2}  = a_1^T \\cdot \\frac{\\delta J}{\\delta z_2} $ , and $\\frac{\\delta J}{\\delta b_2} = [1 ... 1] \\cdot \\frac{\\delta J}{\\delta z_2} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's better understand these two derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Understanding $\\frac{\\delta J}{\\delta z_2}$\n",
    "\n",
    "Both of the derivatives above are dotted with $\\frac{\\delta J}{\\delta z_2}$, because of the chain rule.  As we know, to find the impact from a change in the parameters of $z_2$, we want to assess the change on $z_2$, but also our cost function $J$.  For that we need the chain rule. \n",
    "\n",
    "And notice that we are not, explicitly calculating $\\frac{\\delta J}{\\delta z_2}$.  Why not?  We already calculated it above: $2(z_2 - y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is **backpropagation**.  We calculate the derivative of the outer function $\\frac{\\delta J}{\\delta z_2}$, and then use this already calculated derivative in finding the gradient of the inner layers, via the chain rule.  We'll continue to see this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We're not finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So above, we see the impact of changing the parameters $W_2$ and the bias vector $b_2$.  But take another look at our second linear layer:\n",
    "\n",
    "$$z_2 = a_1W_2 + b_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also have to find the derivative with respect to our inputs $a_1$.  Why?  Well, while it's true that we cannot directly change $a_1$ right here, we will need it when we calculate the derivative of our first linear layer, as changing that first linear layer, will impact the outputs of the activation layer, and so we will need to see the activation layer's impact on $z_2$, and ultimately $J$.  Ok, so here's the derivative of $J$ with respect to $a_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta J}{\\delta a_1} = W_2^T * \\frac{\\delta J}{\\delta z} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Again, we multiply by $\\frac{\\delta J}{\\delta z}$ because of the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Derivative of our activation layer\n",
    "\n",
    "Finally, let's see the derivative of the sigmoid function.  Remember that we use our sigmoid function as our activation layer in our neural network:   $$J(z_2(\\sigma(z_1(X))))$$.\n",
    "\n",
    "Here we assign the sigmoid function to be $a_1(x) = \\sigma(x)$.  Then the derivative of our cost function with respect to $a_1$ is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\frac{\\delta J}{\\delta a_x}= \\sigma(x)*(1 - \\sigma(x)) *\\frac{dJ}{da_1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning this into Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take a look at the code below.  One of the important things to see how we as we move down through the code, we are reusing the derivatives we previously calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(x): return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwards(L1, a1, W2, Y, Y_hat, X):\n",
    "# grad loss\n",
    "    # 2(y_ - y)\n",
    "    dloss = (Y_hat - Y)\n",
    "# grad z2 = a1W2 + b2\n",
    "    #dJ/dW2 =  X.T * dJ/dz_2 \n",
    "    dW2 = (a1.T).dot(dloss)\n",
    "    # dL/db2 = [1]  * dJ/dz_2\n",
    "    db2 = np.sum(dloss, axis=0, keepdims=True)\n",
    "    #dJ/da1 =  dJ/dz_2 * W.T\n",
    "    da1 = dloss.dot(W2.T) \n",
    "\n",
    "# grad sigma            \n",
    "    # dL/dsigma = sig(L1)(1 - sig(L1))*da1 \n",
    "    d_sigma = sigma(L1)*(1 - sigma(L1))*da1\n",
    "# grad z1 = a1W2 + b2  \n",
    "    # dz1/dW1 = dz1/dW1 * dsig/dL1\n",
    "    dW1 = np.dot(X.T, d_sigma)\n",
    "    # dz1/db1 = dz1/db1 * dsig/dL1\n",
    "    db1 = np.sum(d_sigma, axis=0)\n",
    "    \n",
    "    return (dW1, db1, dW2, db2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation in the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important component of the code above is seeing the backpropagation.  Take a look at the code where we find the derivative of the cost function with respect to the parameter of the sigma function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad sigma            \n",
    "    # dL/dsigma = sig(L1)(1 - sig(L1))*da1 \n",
    "    d_sigma = sigma(L1)*(1 - sigma(L1))*da1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the very end we multiply by `da1`, the previously calculated impact of nudging the activation layer.  Now let's look at `da1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "da1 = dloss.dot(W2.T)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`da1` is the $W2^T$ multiplied by `dloss` $\\frac{\\delta J}{dz_2}$.  \n",
    "\n",
    "So the point is that because started at the output layer, and calculated the derivative of each layer, when we get further down in the neural network, applying the chain rule is not so difficult: we have already done the work.  Take a look through the code again to see how this works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we saw how backpropagation occurs in a neural network.  Backpropagation is simply an efficient way of using the chain rule to find the gradients of the parameters of a neural network.  \n",
    "\n",
    "We saw that we can think of the loss function of a neural network as a composite function:\n",
    "\n",
    "$J(\\theta) = (y_i - z_2(\\sigma(z_1(x))))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so to see how to update the weight matrix of a linear layer like $W_2$ we find the derivative of our cost function with respect to each of the functions starting with the outermost function $\\frac{\\delta J}{\\delta z_2}$.  Then we continue to work our way towards the parameters of the input layer.  Because as we move downward towards the input layer, we have already calculated the derivatives up the chain, we do not need to recalculate these derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"https://www.jigsawlabs.io/free\" style=\"position: center\"><img src=\"https://storage.cloud.google.com/curriculum-assets/curriculum-assets.nosync/mom-files/jigsaw-labs.png\" width=\"15%\" style=\"text-align: center\"></a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Code Resources\n",
    "    * [Code derived from wildml's blog post](https://github.com/dennybritz/nn-from-scratch)\n",
    "    * [gradient of bias code explained](https://datascience.stackexchange.com/questions/20139/gradients-for-bias-terms-in-backpropagation)\n",
    "* Softmax Derivatives\n",
    "    * [derivative of softmax](https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function)\n",
    "    * [softmax](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)\n",
    "* Matrix Calculus Resources\n",
    "    * [Excellent matrix calculus guide](http://cs231n.stanford.edu/vecDerivs.pdf)\n",
    "    * [Fast ai matrix calculus](https://explained.ai/matrix-calculus/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
