{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy to Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll begin moving towards the Pytorch library.  We'll see how we can build linear layers in Pytorch as well as make use of non-linear layers.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we've initialized a new model with something like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(n_features, neur_l1, neur_out):\n",
    "    W1 = np.random.randn(n_features, neur_l1) / np.sqrt(n_features)\n",
    "    b1 = np.zeros((1, neur_l1))\n",
    "    W2 = np.random.randn(neur_l1, neur_out) / np.sqrt(neur_l1)\n",
    "    b2 = np.zeros((1, neur_out))\n",
    "    model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "A = np.random.randn(5, 3)\n",
    "B = np.random.randn(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.28608601, -1.32188029,  0.69852584, -1.14986886],\n",
       "       [ 2.72627615,  0.95844272,  0.46377418,  5.63486948],\n",
       "       [-1.28759655, -0.95205668,  0.36545575, -2.02623457],\n",
       "       [-5.18368053, -2.898354  , -0.45369021, -6.02356844],\n",
       "       [-4.1142123 , -1.29720781, -1.79189306, -5.02665616]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A @ B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.zeros(5, 3)\n",
    "\n",
    "A_torch = torch.randn(5, 3)\n",
    "B_torch = torch.randn(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9327, -0.0278,  2.0258,  0.4853,  0.9474],\n",
       "        [-0.1244,  0.3464,  0.8230,  0.1596,  0.2478],\n",
       "        [ 0.9598,  0.1267,  2.8164,  1.1920,  2.0701],\n",
       "        [ 1.0406, -0.4123,  1.1772,  0.6250,  1.1625],\n",
       "        [-1.8487,  0.5782, -2.2509, -0.3788, -0.9299]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_torch @ (B_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn(784, 64) \n",
    "b1 = torch.randn(64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W1.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "fc1 = nn.Linear(784, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =  torch.randn(100, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7982,  0.1649, -0.4013,  ...,  0.6185,  0.2122, -0.7684],\n",
       "        [-0.3963, -0.3520,  0.5135,  ..., -0.0484, -1.0442, -0.1940],\n",
       "        [-0.6166, -0.2651,  0.2134,  ...,  1.5000, -0.7126,  0.8592],\n",
       "        ...,\n",
       "        [ 0.1554, -0.3180,  0.1073,  ..., -1.0696, -0.2473, -0.0074],\n",
       "        [-0.8113,  0.6377,  0.1044,  ..., -0.7322, -0.1972,  0.0618],\n",
       "        [ 1.1190, -0.9516, -0.2273,  ...,  0.3510,  0.3763, -0.3176]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X @ fc1.weight.T + fc1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7982,  0.1649, -0.4013,  ...,  0.6185,  0.2122, -0.7684],\n",
       "        [-0.3963, -0.3520,  0.5135,  ..., -0.0484, -1.0442, -0.1940],\n",
       "        [-0.6166, -0.2651,  0.2134,  ...,  1.5000, -0.7126,  0.8592],\n",
       "        ...,\n",
       "        [ 0.1554, -0.3180,  0.1073,  ..., -1.0696, -0.2473, -0.0074],\n",
       "        [-0.8113,  0.6377,  0.1044,  ..., -0.7322, -0.1972,  0.0618],\n",
       "        [ 1.1190, -0.9516, -0.2273,  ...,  0.3510,  0.3763, -0.3176]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Module:\n",
    "#     def __init__(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ??nn.Module\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(784, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear.weight, linear.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(784, 64)\n",
    "        self.W2 = nn.Linear(64, 64)\n",
    "        self.W3 = nn.Linear(64, 64)\n",
    "        self.W4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        Z1 = self.W1(X)\n",
    "        A1 = torch.sigmoid(Z1)\n",
    "        Z2 = self.W2(A1)\n",
    "        A2 = torch.sigmoid(Z2)\n",
    "        Z3 = self.W3(A2)\n",
    "        A3 = torch.sigmoid(Z3)\n",
    "        Z4 = self.W4(A3)\n",
    "        return F.log_softmax(Z4, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (W1): Linear(in_features=784, out_features=64, bias=True)\n",
       "  (W2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (W3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (W4): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_net = Net()\n",
    "neural_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6, 12])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6*(torch.tensor([1, 2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(100, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3801, -2.1525, -2.1782, -2.2217, -2.1163, -2.3503, -2.3163, -2.5885,\n",
       "         -2.6359, -2.2224],\n",
       "        [-2.3811, -2.1522, -2.1782, -2.2208, -2.1160, -2.3489, -2.3160, -2.5907,\n",
       "         -2.6343, -2.2241],\n",
       "        [-2.3795, -2.1537, -2.1773, -2.2208, -2.1149, -2.3516, -2.3154, -2.5896,\n",
       "         -2.6345, -2.2248],\n",
       "        [-2.3812, -2.1533, -2.1767, -2.2221, -2.1144, -2.3483, -2.3152, -2.5897,\n",
       "         -2.6363, -2.2255],\n",
       "        [-2.3788, -2.1514, -2.1783, -2.2165, -2.1148, -2.3527, -2.3169, -2.5912,\n",
       "         -2.6368, -2.2264],\n",
       "        [-2.3782, -2.1536, -2.1760, -2.2212, -2.1157, -2.3526, -2.3148, -2.5896,\n",
       "         -2.6367, -2.2243],\n",
       "        [-2.3784, -2.1528, -2.1768, -2.2201, -2.1173, -2.3510, -2.3165, -2.5888,\n",
       "         -2.6372, -2.2236],\n",
       "        [-2.3809, -2.1524, -2.1776, -2.2196, -2.1132, -2.3533, -2.3155, -2.5898,\n",
       "         -2.6360, -2.2249],\n",
       "        [-2.3799, -2.1517, -2.1789, -2.2201, -2.1139, -2.3517, -2.3155, -2.5893,\n",
       "         -2.6359, -2.2259],\n",
       "        [-2.3785, -2.1562, -2.1751, -2.2233, -2.1155, -2.3513, -2.3155, -2.5902,\n",
       "         -2.6374, -2.2200],\n",
       "        [-2.3766, -2.1505, -2.1786, -2.2203, -2.1161, -2.3515, -2.3184, -2.5911,\n",
       "         -2.6353, -2.2243],\n",
       "        [-2.3802, -2.1526, -2.1777, -2.2202, -2.1152, -2.3526, -2.3168, -2.5894,\n",
       "         -2.6360, -2.2223],\n",
       "        [-2.3800, -2.1518, -2.1781, -2.2199, -2.1158, -2.3535, -2.3154, -2.5898,\n",
       "         -2.6351, -2.2232],\n",
       "        [-2.3799, -2.1506, -2.1772, -2.2216, -2.1139, -2.3521, -2.3145, -2.5911,\n",
       "         -2.6375, -2.2257],\n",
       "        [-2.3790, -2.1529, -2.1798, -2.2188, -2.1140, -2.3526, -2.3172, -2.5875,\n",
       "         -2.6354, -2.2249],\n",
       "        [-2.3788, -2.1552, -2.1794, -2.2202, -2.1170, -2.3500, -2.3158, -2.5887,\n",
       "         -2.6348, -2.2213],\n",
       "        [-2.3792, -2.1518, -2.1768, -2.2192, -2.1161, -2.3532, -2.3166, -2.5889,\n",
       "         -2.6367, -2.2245],\n",
       "        [-2.3802, -2.1506, -2.1808, -2.2200, -2.1138, -2.3520, -2.3157, -2.5892,\n",
       "         -2.6350, -2.2254],\n",
       "        [-2.3785, -2.1535, -2.1785, -2.2196, -2.1128, -2.3543, -2.3176, -2.5894,\n",
       "         -2.6370, -2.2222],\n",
       "        [-2.3783, -2.1522, -2.1788, -2.2219, -2.1147, -2.3473, -2.3202, -2.5883,\n",
       "         -2.6360, -2.2245],\n",
       "        [-2.3782, -2.1523, -2.1819, -2.2186, -2.1161, -2.3487, -2.3176, -2.5912,\n",
       "         -2.6318, -2.2248],\n",
       "        [-2.3784, -2.1534, -2.1792, -2.2182, -2.1169, -2.3535, -2.3165, -2.5909,\n",
       "         -2.6329, -2.2220],\n",
       "        [-2.3789, -2.1547, -2.1768, -2.2216, -2.1184, -2.3488, -2.3155, -2.5909,\n",
       "         -2.6332, -2.2224],\n",
       "        [-2.3811, -2.1518, -2.1797, -2.2193, -2.1149, -2.3517, -2.3141, -2.5899,\n",
       "         -2.6362, -2.2243],\n",
       "        [-2.3784, -2.1537, -2.1800, -2.2177, -2.1174, -2.3493, -2.3173, -2.5889,\n",
       "         -2.6334, -2.2249],\n",
       "        [-2.3813, -2.1533, -2.1788, -2.2210, -2.1137, -2.3501, -2.3152, -2.5876,\n",
       "         -2.6363, -2.2250],\n",
       "        [-2.3819, -2.1513, -2.1785, -2.2215, -2.1147, -2.3509, -2.3139, -2.5908,\n",
       "         -2.6343, -2.2250],\n",
       "        [-2.3780, -2.1504, -2.1770, -2.2203, -2.1172, -2.3522, -2.3168, -2.5905,\n",
       "         -2.6354, -2.2249],\n",
       "        [-2.3814, -2.1527, -2.1788, -2.2226, -2.1165, -2.3496, -2.3148, -2.5880,\n",
       "         -2.6347, -2.2225],\n",
       "        [-2.3813, -2.1536, -2.1765, -2.2220, -2.1152, -2.3518, -2.3137, -2.5907,\n",
       "         -2.6363, -2.2221],\n",
       "        [-2.3791, -2.1518, -2.1777, -2.2219, -2.1150, -2.3521, -2.3166, -2.5889,\n",
       "         -2.6347, -2.2245],\n",
       "        [-2.3783, -2.1528, -2.1799, -2.2183, -2.1150, -2.3518, -2.3163, -2.5900,\n",
       "         -2.6359, -2.2243],\n",
       "        [-2.3798, -2.1534, -2.1814, -2.2181, -2.1145, -2.3505, -2.3176, -2.5896,\n",
       "         -2.6333, -2.2236],\n",
       "        [-2.3808, -2.1508, -2.1799, -2.2190, -2.1154, -2.3513, -2.3161, -2.5885,\n",
       "         -2.6344, -2.2257],\n",
       "        [-2.3772, -2.1549, -2.1779, -2.2205, -2.1171, -2.3501, -2.3191, -2.5883,\n",
       "         -2.6343, -2.2216],\n",
       "        [-2.3783, -2.1555, -2.1766, -2.2207, -2.1177, -2.3475, -2.3150, -2.5899,\n",
       "         -2.6349, -2.2252],\n",
       "        [-2.3801, -2.1514, -2.1791, -2.2202, -2.1127, -2.3538, -2.3141, -2.5899,\n",
       "         -2.6354, -2.2266],\n",
       "        [-2.3809, -2.1520, -2.1783, -2.2217, -2.1135, -2.3500, -2.3168, -2.5907,\n",
       "         -2.6344, -2.2245],\n",
       "        [-2.3770, -2.1495, -2.1801, -2.2204, -2.1137, -2.3516, -2.3185, -2.5902,\n",
       "         -2.6365, -2.2258],\n",
       "        [-2.3815, -2.1517, -2.1798, -2.2203, -2.1159, -2.3499, -2.3144, -2.5886,\n",
       "         -2.6348, -2.2250],\n",
       "        [-2.3791, -2.1524, -2.1758, -2.2234, -2.1162, -2.3509, -2.3159, -2.5882,\n",
       "         -2.6349, -2.2250],\n",
       "        [-2.3808, -2.1507, -2.1805, -2.2186, -2.1132, -2.3523, -2.3168, -2.5900,\n",
       "         -2.6368, -2.2240],\n",
       "        [-2.3779, -2.1502, -2.1774, -2.2221, -2.1170, -2.3512, -2.3197, -2.5887,\n",
       "         -2.6365, -2.2219],\n",
       "        [-2.3796, -2.1489, -2.1791, -2.2200, -2.1134, -2.3532, -2.3153, -2.5908,\n",
       "         -2.6368, -2.2268],\n",
       "        [-2.3792, -2.1509, -2.1767, -2.2222, -2.1151, -2.3533, -2.3147, -2.5904,\n",
       "         -2.6373, -2.2240],\n",
       "        [-2.3808, -2.1516, -2.1784, -2.2208, -2.1146, -2.3527, -2.3144, -2.5894,\n",
       "         -2.6361, -2.2241],\n",
       "        [-2.3796, -2.1521, -2.1773, -2.2190, -2.1143, -2.3533, -2.3155, -2.5913,\n",
       "         -2.6375, -2.2242],\n",
       "        [-2.3798, -2.1536, -2.1799, -2.2178, -2.1150, -2.3499, -2.3137, -2.5914,\n",
       "         -2.6371, -2.2250],\n",
       "        [-2.3814, -2.1529, -2.1806, -2.2205, -2.1156, -2.3498, -2.3151, -2.5880,\n",
       "         -2.6344, -2.2232],\n",
       "        [-2.3760, -2.1509, -2.1766, -2.2215, -2.1162, -2.3490, -2.3170, -2.5918,\n",
       "         -2.6387, -2.2259],\n",
       "        [-2.3792, -2.1517, -2.1778, -2.2203, -2.1156, -2.3495, -2.3160, -2.5906,\n",
       "         -2.6362, -2.2259],\n",
       "        [-2.3811, -2.1531, -2.1779, -2.2213, -2.1155, -2.3506, -2.3154, -2.5878,\n",
       "         -2.6350, -2.2242],\n",
       "        [-2.3810, -2.1524, -2.1783, -2.2226, -2.1174, -2.3477, -2.3181, -2.5892,\n",
       "         -2.6334, -2.2213],\n",
       "        [-2.3794, -2.1513, -2.1796, -2.2180, -2.1149, -2.3534, -2.3159, -2.5925,\n",
       "         -2.6365, -2.2225],\n",
       "        [-2.3794, -2.1512, -2.1810, -2.2196, -2.1157, -2.3511, -2.3147, -2.5887,\n",
       "         -2.6369, -2.2242],\n",
       "        [-2.3819, -2.1542, -2.1774, -2.2223, -2.1137, -2.3508, -2.3151, -2.5884,\n",
       "         -2.6362, -2.2227],\n",
       "        [-2.3805, -2.1540, -2.1770, -2.2197, -2.1183, -2.3500, -2.3170, -2.5872,\n",
       "         -2.6347, -2.2229],\n",
       "        [-2.3794, -2.1503, -2.1769, -2.2185, -2.1135, -2.3571, -2.3147, -2.5925,\n",
       "         -2.6386, -2.2240],\n",
       "        [-2.3774, -2.1500, -2.1804, -2.2182, -2.1129, -2.3538, -2.3177, -2.5929,\n",
       "         -2.6362, -2.2247],\n",
       "        [-2.3780, -2.1528, -2.1785, -2.2220, -2.1163, -2.3481, -2.3169, -2.5883,\n",
       "         -2.6348, -2.2256],\n",
       "        [-2.3807, -2.1538, -2.1784, -2.2193, -2.1157, -2.3514, -2.3156, -2.5911,\n",
       "         -2.6338, -2.2226],\n",
       "        [-2.3808, -2.1510, -2.1790, -2.2202, -2.1151, -2.3528, -2.3130, -2.5897,\n",
       "         -2.6364, -2.2248],\n",
       "        [-2.3807, -2.1501, -2.1801, -2.2184, -2.1137, -2.3529, -2.3165, -2.5898,\n",
       "         -2.6357, -2.2256],\n",
       "        [-2.3778, -2.1522, -2.1793, -2.2204, -2.1140, -2.3509, -2.3168, -2.5881,\n",
       "         -2.6353, -2.2270],\n",
       "        [-2.3794, -2.1516, -2.1810, -2.2197, -2.1141, -2.3496, -2.3162, -2.5907,\n",
       "         -2.6361, -2.2246],\n",
       "        [-2.3786, -2.1517, -2.1796, -2.2198, -2.1142, -2.3520, -2.3177, -2.5891,\n",
       "         -2.6343, -2.2251],\n",
       "        [-2.3772, -2.1515, -2.1788, -2.2204, -2.1137, -2.3529, -2.3155, -2.5916,\n",
       "         -2.6382, -2.2243],\n",
       "        [-2.3766, -2.1511, -2.1795, -2.2180, -2.1141, -2.3551, -2.3172, -2.5912,\n",
       "         -2.6363, -2.2246],\n",
       "        [-2.3795, -2.1507, -2.1778, -2.2196, -2.1147, -2.3561, -2.3148, -2.5880,\n",
       "         -2.6358, -2.2258],\n",
       "        [-2.3784, -2.1502, -2.1802, -2.2213, -2.1150, -2.3484, -2.3187, -2.5907,\n",
       "         -2.6369, -2.2232],\n",
       "        [-2.3798, -2.1515, -2.1758, -2.2221, -2.1140, -2.3523, -2.3135, -2.5908,\n",
       "         -2.6396, -2.2250],\n",
       "        [-2.3798, -2.1494, -2.1800, -2.2212, -2.1145, -2.3522, -2.3154, -2.5895,\n",
       "         -2.6351, -2.2256],\n",
       "        [-2.3789, -2.1505, -2.1767, -2.2223, -2.1146, -2.3504, -2.3155, -2.5906,\n",
       "         -2.6382, -2.2261],\n",
       "        [-2.3795, -2.1541, -2.1752, -2.2235, -2.1160, -2.3465, -2.3159, -2.5890,\n",
       "         -2.6362, -2.2260],\n",
       "        [-2.3815, -2.1542, -2.1783, -2.2186, -2.1168, -2.3506, -2.3164, -2.5888,\n",
       "         -2.6334, -2.2230],\n",
       "        [-2.3809, -2.1493, -2.1794, -2.2214, -2.1131, -2.3510, -2.3167, -2.5895,\n",
       "         -2.6386, -2.2243],\n",
       "        [-2.3793, -2.1514, -2.1802, -2.2196, -2.1139, -2.3504, -2.3147, -2.5914,\n",
       "         -2.6357, -2.2263],\n",
       "        [-2.3803, -2.1529, -2.1782, -2.2180, -2.1146, -2.3537, -2.3152, -2.5891,\n",
       "         -2.6359, -2.2249],\n",
       "        [-2.3815, -2.1533, -2.1797, -2.2184, -2.1122, -2.3541, -2.3141, -2.5904,\n",
       "         -2.6342, -2.2251],\n",
       "        [-2.3800, -2.1528, -2.1765, -2.2216, -2.1149, -2.3512, -2.3165, -2.5902,\n",
       "         -2.6353, -2.2238],\n",
       "        [-2.3819, -2.1524, -2.1778, -2.2199, -2.1143, -2.3530, -2.3134, -2.5908,\n",
       "         -2.6370, -2.2234],\n",
       "        [-2.3788, -2.1502, -2.1803, -2.2174, -2.1136, -2.3548, -2.3144, -2.5923,\n",
       "         -2.6367, -2.2258],\n",
       "        [-2.3792, -2.1534, -2.1752, -2.2213, -2.1176, -2.3503, -2.3149, -2.5938,\n",
       "         -2.6370, -2.2212],\n",
       "        [-2.3793, -2.1532, -2.1785, -2.2210, -2.1137, -2.3523, -2.3161, -2.5906,\n",
       "         -2.6350, -2.2231],\n",
       "        [-2.3829, -2.1491, -2.1802, -2.2190, -2.1146, -2.3534, -2.3123, -2.5905,\n",
       "         -2.6355, -2.2259],\n",
       "        [-2.3796, -2.1521, -2.1800, -2.2200, -2.1151, -2.3495, -2.3139, -2.5907,\n",
       "         -2.6371, -2.2250],\n",
       "        [-2.3793, -2.1521, -2.1752, -2.2218, -2.1140, -2.3524, -2.3150, -2.5926,\n",
       "         -2.6366, -2.2252],\n",
       "        [-2.3813, -2.1544, -2.1775, -2.2222, -2.1138, -2.3512, -2.3146, -2.5880,\n",
       "         -2.6367, -2.2230],\n",
       "        [-2.3791, -2.1529, -2.1766, -2.2222, -2.1145, -2.3518, -2.3165, -2.5897,\n",
       "         -2.6365, -2.2234],\n",
       "        [-2.3800, -2.1544, -2.1768, -2.2218, -2.1162, -2.3511, -2.3134, -2.5898,\n",
       "         -2.6359, -2.2230],\n",
       "        [-2.3796, -2.1558, -2.1756, -2.2200, -2.1152, -2.3521, -2.3149, -2.5920,\n",
       "         -2.6341, -2.2234],\n",
       "        [-2.3779, -2.1505, -2.1798, -2.2222, -2.1154, -2.3495, -2.3178, -2.5888,\n",
       "         -2.6340, -2.2256],\n",
       "        [-2.3777, -2.1508, -2.1785, -2.2183, -2.1161, -2.3496, -2.3185, -2.5893,\n",
       "         -2.6367, -2.2271],\n",
       "        [-2.3798, -2.1536, -2.1781, -2.2195, -2.1162, -2.3512, -2.3152, -2.5891,\n",
       "         -2.6366, -2.2232],\n",
       "        [-2.3793, -2.1522, -2.1779, -2.2201, -2.1185, -2.3487, -2.3160, -2.5910,\n",
       "         -2.6351, -2.2233],\n",
       "        [-2.3799, -2.1517, -2.1791, -2.2207, -2.1149, -2.3512, -2.3158, -2.5896,\n",
       "         -2.6363, -2.2237],\n",
       "        [-2.3799, -2.1523, -2.1776, -2.2219, -2.1131, -2.3501, -2.3173, -2.5907,\n",
       "         -2.6351, -2.2252],\n",
       "        [-2.3783, -2.1533, -2.1793, -2.2210, -2.1165, -2.3495, -2.3170, -2.5879,\n",
       "         -2.6347, -2.2237],\n",
       "        [-2.3756, -2.1503, -2.1780, -2.2175, -2.1182, -2.3528, -2.3193, -2.5918,\n",
       "         -2.6360, -2.2236],\n",
       "        [-2.3773, -2.1504, -2.1795, -2.2197, -2.1147, -2.3522, -2.3176, -2.5889,\n",
       "         -2.6368, -2.2257]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .0005*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialized a set of weight matrices (`W1`, and `W2`) and bias vectors (`b1` and `b2`), which we later tune through our training procedure.  Once these weights and biases are trained, we can use them to make a prediction with our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Pytorch, we can do the same thing with a Pytorch tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "W1 = torch.randn(8, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1513, -0.0768, -0.3163,  ..., -1.3084, -0.2952, -1.2212],\n",
       "        [ 0.5460,  0.0042,  1.0023,  ...,  1.1528,  0.1197,  0.0430],\n",
       "        [ 0.4627, -1.6576, -2.6115,  ...,  0.6369, -1.8958,  1.0517],\n",
       "        ...,\n",
       "        [ 0.4963, -0.0175,  0.3728,  ..., -2.6316,  0.1751,  0.4416],\n",
       "        [ 0.6346,  1.1451,  0.7053,  ...,  0.6085,  0.0940, -1.5013],\n",
       "        [-0.2043,  1.0081,  0.3093,  ..., -1.3962,  0.3015,  0.8347]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 784])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the Pytorch library provides us with much of the same interface as numpy.  We can initialize data through the `randn` function, the `randint` function, or by creating our own custom array just like in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can select our data by using the same bracket operators as we saw in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1513, -0.0768, -0.3163],\n",
       "        [ 0.5460,  0.0042,  1.0023]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1[:2, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference between a pytorch tensor and a numpy array that we need to worry about at this point is reshaping our data.  In Pytorch, the method for changing the shape of the data is called `view`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1513, -0.0768, -0.3163,  ..., -1.3962,  0.3015,  0.8347])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Operations in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of neural networks, perhaps the most important thing to know is how to perform matrix algebra operations.  Remember that the whole point of initializing a weight matrix and a bias vector is to use them to calculate the linear output of each neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 784])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = torch.randn(3, 784)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 784]), torch.Size([784, 3]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.shape, X.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -8.4259, -11.8295,  23.4698],\n",
       "        [-39.4412,  36.8929, -61.9721],\n",
       "        [-23.1415,  42.7091,  25.7792],\n",
       "        [ 33.3887, -11.9642,  13.4949],\n",
       "        [ 22.8652,   8.9591,  -3.2612],\n",
       "        [ -9.7971,  17.3862, -10.0517],\n",
       "        [ 12.3032, -52.7073,  32.4848],\n",
       "        [ 19.9870,  23.3461,  -0.5489]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 @ X.T \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see above that we have eight outputs for each of our three observations, just like we saw before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to include the addition of a bias vector, we initialize a vector with eight rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0904],\n",
       "        [-1.2224],\n",
       "        [-0.6987],\n",
       "        [ 0.1317],\n",
       "        [ 1.6392],\n",
       "        [ 1.4074],\n",
       "        [ 0.4002],\n",
       "        [-0.1969]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = torch.randn(8, 1)\n",
    "b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10.5163, -13.9199,  21.3794],\n",
       "        [-40.6635,  35.6705, -63.1945],\n",
       "        [-23.8402,  42.0104,  25.0805],\n",
       "        [ 33.5204, -11.8325,  13.6266],\n",
       "        [ 24.5044,  10.5983,  -1.6220],\n",
       "        [ -8.3896,  18.7937,  -8.6443],\n",
       "        [ 12.7034, -52.3072,  32.8849],\n",
       "        [ 19.7900,  23.1491,  -0.7459]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z1 = W1 @ X.T  + b1\n",
    "Z1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So at this point, we can translate our `init_model` function to use Pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(n_features, neur_l1, neur_out):\n",
    "    W1 = torch.randn(n_features, neur_l1) / np.sqrt(n_features)\n",
    "    b1 = torch.zeros((1, neur_l1))\n",
    "    W2 = torch.randn(neur_l1, neur_out) / np.sqrt(neur_l1)\n",
    "    b2 = torch.zeros((1, neur_out))\n",
    "    model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': tensor([[ 0.2883,  0.5991,  0.0466],\n",
       "         [-0.5802, -0.3250, -0.8185],\n",
       "         [ 0.4495, -0.1099,  0.4779],\n",
       "         [ 0.0704, -0.6322,  0.9711],\n",
       "         [ 0.7418, -0.4575, -0.1500]]),\n",
       " 'b1': tensor([[0., 0., 0.]]),\n",
       " 'W2': tensor([[-0.0307, -0.4393],\n",
       "         [-0.5518, -0.3604],\n",
       "         [-0.3678, -0.2677]]),\n",
       " 'b2': tensor([[0., 0.]])}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_model(5, 3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we learned about initializing the weight matrices and bias vectors of our neural network with Pytorch tensors.  As we saw, Pytorch tensors are similar to numpy arrays both in their construction and the operations we can perform with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can initialize a Pytorch tensor in similar ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "W1 = torch.randn(8, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 784])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `view` method to reshape the tensore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2966, -0.7510,  0.9930,  ...,  1.9472, -0.3612,  0.0994])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And can perform matrix algebra operations with our tensor to pass our data through our linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.1926e+00,  2.8639e-03, -8.8633e+00],\n",
       "        [ 4.1558e+01, -3.9941e+01, -3.0199e+01],\n",
       "        [-1.3246e+01,  1.8456e+01,  2.3438e+01],\n",
       "        [-3.0452e+01, -2.8885e+01, -2.9598e+01],\n",
       "        [ 5.8378e+00, -4.2085e+01,  1.7439e+01],\n",
       "        [-1.4112e+01, -2.5177e+00,  3.6517e+01],\n",
       "        [-1.4192e+00, -3.1555e+01, -7.2901e+00],\n",
       "        [ 2.5637e+01,  5.2545e+01,  1.7124e+01]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z1 = W1 @ X.T  + b1\n",
    "Z1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
